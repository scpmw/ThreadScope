\documentclass[twocolumn,9pt]{sigplanconf}

\usepackage{url}
% \usepackage{code}
\usepackage{graphicx}
\usepackage{enumerate}

\usepackage{listings}
\lstset{basicstyle=\fontfamily{cmss} \small, columns=fullflexible, language=Haskell, numbers=left, numberstyle=\tiny, numbersep=2pt}

\newcommand{\codef}[1]{{\fontfamily{cmss}\small#1}}
\newcommand{\boldcode}[1]{{\bf\fontfamily{cmss}\small#1}}

\usepackage{natbib}
\bibpunct();A{},
\let\cite=\citep

\nocaptionrule

\title{Parallel Performance Tuning for Haskell}

\authorinfo{Donnie Jones}{University of Kentucky}
           {donnie@darthik.com}
\authorinfo{Simon Marlow}{Microsoft Research}
           {simonmar@microsoft.com}
\authorinfo{Satnam Singh}{Microsoft Research}
           {satnams@microsoft.com}

\begin{document}

\maketitle
%\makeatactive

\begin{abstract}
Parallel Haskell programming has entered the mainstream with support
now included in GHC for multiple parallel programming models, along
with multicore execution support in the runtime.  However, tuning
programs for parallelism is still something of a black art.  Without
much in the way of feedback provided by the runtime system, it is a
matter of trial and error combined with experience to achieve good
parallel speedups.

This paper describes an early prototype of a parallel profiling system
for multicore programming with GHC.  The system comprises three parts:
fast event tracing in the runtime, a Haskell library for reading the
resulting trace files, and a number of tools built on this library for
presenting the information to the programmer.  We focus on one tool in
particular, a graphical timeline browser called ThreadScope.

The paper illustrates the use of ThreadScope through a number of case
studies, and describes some useful methodologies for parallelising
Haskell programs.
\end{abstract}


\section{Introduction}

Life has never been better for the Parallel Haskell programmer: GHC
supports multicore execution out of the box, including multiple
parallel programming models: Strategies \cite{spj:trin98b}, Concurrent
Haskell \cite{conchask} with STM \cite{stm}, and Data Parallel Haskell
\cite{dph}.  Performance of the runtime system has received concerted
attention recently, with significant improvements in parallel
performance coming in forthcoming GHC releases \cite{multicore-ghc}.
Many of the runtime bottlenecks that hampered parallel performance in
earlier GHC versions are much reduced, with the result that it should
now be easier to achieve parallel speedups.

However, optimising the runtime only addresses half of the problem;
the other half being how to tune a given Haskell program to run
effectively in parallel.  The programmer still has control over task
granularity, data dependencies, speculation, and to some extent
evaluation order.  Getting these wrong can be disastrous for parallel
performance. For example the granularity should neither be too fine
nor too coarse.  The runtime does everything it can to broaden the
range of granularities that yield good performance, but it cannot
completely eliminate the upper and lower bounds.

Current methods for tuning parallel Haskell programs rely largely on
trial and error, experience, and an eye for understanding the limited
statistics produced at the end of a program's run by the runtime
system.  What we need are effective ways to measure and collect
information about the runtime behaviour of parallel Haskell programs,
and tools to communicate this information to the programmer in a
way that they can understand and use to solve performance problems
with their programs.

In this paper we desribe a new profiling system developed for the
purposes of understanding the parallel execution of Haskell programs.
In particular, our system includes a tool called ThreadScope that
allows the programmer to interactively browse the parallel execution
profile.

This paper contributes the following:

\begin{itemize}
\item We describe the design of our parallel profiling system, and
  the ThreadScope tool for understanding parallel execution.  Our
  trace file format is fully extensible, and profiling tools built
  using our framework are both backwards- and forward-compatible with
  different versions of GHC.

\item Through several case studies, we explore how to use ThreadScope
  for identifying parallel performance problems, and describe a
  selection of methodologies for parallelising Haskell code.
\end{itemize}

Earlier methodologies for parallelising Haskell code exist
\cite{spj:trin98b}, but there are two crucial differences in the
multicore GHC setting.  Firstly, the tradeoffs are likely to be
different, since we are working with a shared-memory heap, and
communication is therefore cheap\footnote{though not entirely free,
  since memory cache hierarchies mean data still has to be shuffled
  between processors even if that shuffling is not explicitly
  programmed.}.  Secondly, it has recently been discovered that
Strategies interact badly with garbage collection
\cite{multicore-ghc}, so in this paper we avoid the use of the
original Strategies library, relying instead on our own simple
hand-rolled parallel combinators.

Our work is at an early stage.  The ThreadScope tool displays only one
particular view of the execution of Parallel Haskell programs (albeit
a very useful one).  There are a wealth of possibilities, both for
improving ThreadScope itself and for building new tools.  We cover
some of the possibilities in Section~\ref{s:conclusion}.

\input{motivation}

\section{Profiling Infrastructure}
Our profiling framework comprises three parts:

\begin{itemize}
\item Support in GHC's runtime for tracing events to a log file at
  runtime.  The tracing is designed to be as lightweight as possible,
  so as not to have any significant impact on the behaviour of the
  program being measured.

\item A Haskell library \codef{ghc-events} that can read the trace file
  generated by the runtime and build a Haskell data structure
  representing the trace.

\item Multiple tools make use of the \codef{ghc-events} library to read and
  analyse trace files.
\end{itemize}

Having a single trace-file format and a library that parses it means
that it is easy to write a new tool that works with GHC trace files:
just import the \codef{ghc-events} package and write code that uses the
Haskell data structures directly.  We have already built several such
tools ourselves, some of which are merely proof-of-concept
experiments, but the \codef{ghc-events} library makes it almost trivial to
create new tools:

\begin{itemize}
\item A simple program that just prints out the (sorted) contents of
  the trace file as text.  Useful for checking that a trace file can
  be parsed, and for examining the exact sequence of events.

\item The ThreadScope graphical viewer.

\item A tool that parses a trace file and generates a PDF format
  timeline view, similar to the ThreadScope view.

\item A tool that generates input in the format expected by the
  gtkwave circuit waveform viewer.  This was used as an early
  prototype for ThreadScope, since the timeline view that we want to
  display has a lot in common with the waveform diagrams that gtkwave
  displays and browses.
\end{itemize}

\subsection{Fast runtime tracing}
Minimizing the performance overhead is critical when designing a system
to facilitate runtime tracing.  In the GHC runtime, a pre-allocated events buffer is 
used by each capability to store generated events.
By doing so, we prevent any performance delays from dynamic 
memory allocation, and require no need for locks since the buffers are
capability-specific.  Yet, this requires us to flush the buffer
to the filesystem once it becomes full, but since the buffer is a
constant size we pay a near-constant penalty for each flush and a
deterministic delay on the GHC runtime.

% how fast can we generate an event?
From the event timestamps in the trace log file, we can verify that we can 
generate an event within 1 ms.  However, we can also see from the log file that
the typical duration between events is greater than 1 ms due to the computation 
required for the event itself, thus the event generation is within
the computation time for even very fast events, such as create spark.

% how many events are there in a typical log file?
To provide an example of the impact of GHC runtime event tracing, the parallel 
version of the canonical Fibonacci function, \codef{parfib} (from
the \codef{nofib} software package \cite{nofib}) typically generates over
2,000,000 events in a trace file with size of approximately 40 MB for the
calculation of the Fibonacci number 40 at threshold 10 and utilizing 2
CPU cores

% how much impact does this have on runtimes?
Taking this parfib computation further, we compare the time elapsed with
event logging enabled and without event logging for 50 executions of
parfib on an Intel(R) Core(TM)2 Duo CPU T5250 1.50GHz.

\begin{verbatim}
  parfib eventlog 
  ./Main 40 10 +RTS -N2 -l -RTS
  Avg Time Elapsed  Standard Deviation
  20.582757s        0.789547s

  parfib without eventlog 
  ./Main 40 10 +RTS -N2 -RTS
  Avg Time Elapsed  Standard Deviation
  17.447493s        1.352686s
\end{verbatim}

Considering the significant number of events generated in the traces and
the very detailed profiling information made available by these traces, 
the overhead does not have an immense impact at approximately 10-25\%
increase in elapsed time.  Yet, if we can post an event in 1ms, why event 
the 10-25\% overhead? In the case of parfib, the create spark event
consists of at least 80\% of the 
the events generated, and many sparks are often created 
one after another to create new work for the capabilities.  So, we see
in this case we have a higher overhead due to the behavior of the program
itself not doing much work but rather spending much of the computation
creating sparks.
  
For parallel quicksort, far fewer sparks are created and most of the
computation is spent in garbage collection; thus, we can achieve an
almost unnoticable overhead from event tracing.  The parallel quicksort
example involved sorting a list of 100,000 randomly generated integers
and was performed in the same manner as parfib where we compare with
event logging and without, yet in this test we perform 100 executions 
on an Intel(R) Core(TM) 2 Quad CPU 3.0Ghz. 

\begin{verbatim}
  parquicksort eventlog 
  ./Main +RTS -N4 -l -RTS 
  Avg Time Elapsed  Standard Deviation
  14.201385s        2.954869

  parquicksort without eventlog 
  ./Main +RTS -N4 -RTS 
  Avg Time Elapsed  Standard Deviation
  15.187529s        3.385293s

\end{verbatim}

Since parallel quicksort spent the majority of the computation doing
useful work, particularly garbage collection of the created lists, a
trace file of only approximately 5MB and near 300,000 events was
created and the overhead of event tracing is not noticeable.

The crux of the event tracing is that even when a poorly performing
program utilizes event tracing, the overhead should still not be
devastating to the program's performance, but best of all on a program
with high utilization event tracing should barely affect the performance.

\subsection{An extensible file format}

We believe it is essential that the trace file format is both
backwards and forwards compatible, and architecture independent.  In
particular, this means that:

\begin{itemize}
\item If you build a newer version of a tool, it will still work with
  the trace files you already have, and trace files generated by
  programs compiled with older versions of GHC.

\item If you upgrade your GHC and recompile your programs, the trace
  files will still work with any profiling tools you already have.

\item Trace files do not have a shelf life.  You can keep your trace
  files around, safe in the knowledge that they will work with future
  versions of profiling tools.  Trace files can be archived, and
  shared between machines.
\end{itemize}

Nevertheless, we don't expect the form of trace files to remain
completely static.  In the future we will certainly want to add new
events, and add more information to existing events.  We therefore
need an extensible file format.  Informally, our trace files are
structured as follows:

\begin{itemize}
\item A list of \emph{event types}.  An event-type is a
  variable-length structure that describes one kind of event.  The
  event-type structure contains
  \begin{itemize}
    \item A unique number for this event type
    \item A field describing the length in bytes of an instance of the
      event, or zero for a variable-length event.
    \item A variable-length string (preceded by its length) describing
      this event (for example ``thread created'')
    \item A variable-length field (preceded by its length) for future
      expansion.  We might in the future want to add more fields to
      the event-type structure, and this field allows for that.
  \end{itemize}
\item A list of \emph{events}.  Each event begins with an event number
  that corresponds to one of the event types defined earlier, and the
  length of the event structure is given by the event type (or it has
  variable length).  The event also contains
  \begin{itemize}
  \item A nanosecond-resolution timestamp.
  \item For a variable-length event, the length of the event.
  \item Information specific to this event, for example which CPU it
    occurred on.  If the parser knows about this event, then it can
    parse the rest of the event's information, otherwise it can skip
    over this field because its length is known.
  \end{itemize}
\end{itemize}

The unique numbers that identify events are shared knowledge between
GHC and the \codef{ghc-events} library.  When creating a new event, a new
unique identifier is chosen; identifiers can never be re-used.

Even when parsing a trace file that contains new events, the parser
can still give a timestamp and a description of the unknown events.
The parser might encounter an event-type that it knows about, but the
event-type might contain new unknown fields.  The parser can recognise
this situation and skip over the extra fields, because it knows the
length of the event from the event-type structure.  Therefore when a
tool encounters a new log file it can continue to provide consistent
functionality.

Of course, there are scenarios in which it isn't possible to provide
this ideal graceful degradation.  For example, we might construct a
tool that profiles a particular aspect of the behaviour of the
runtime, and in the future the runtime might be redesigned to behave
in a completely different way, with a new set of events.  The old
events simply won't be generated any more, and the old tool won't be
able to display anything useful with the new trace files.  Still, we
expect that our extensible trace file format will allows us to smooth
over the majority of forwards- and backwards-compatibility issues that
will arise between versions of the tools and GHC runtime.  Moreover,
extensibility costs almost nothing, since the extra fields are all in
the event-types header, which has a fixed size for a given version of
GHC.

\input{bsort}


\subsection{Soda}

Soda is a program for solving word-search problems: given a
rectangular grid of letters, find occurrences of a words from a
supplied list, where a word can appear horizonally, vertically, or
diagnoally, in either direction (giving a total of eight possible
orientations).

The program has a long history as a Parallel Haskell benchmark
\cite{soda}.  The version we start with here is a recent incarnation,
using a random initial grid with a tunable size.  The words do not in
fact appear in the grid; the program just fruitlessly searches the
entire grid for a predefined list of words.  One advantage of this
formulation for benchmark purposes is that the program's performance
does not depend on the search order, however a disadvantage is that
the parallel structure is unrealistically regular.

The parallelism is expressed using \codef{parListWHNF} \cite{multicore-ghc}
to avoid the space leak issues with the standard strategy
implementation of \codef{parList}.

To establish the baseline performance, we run the program using GHC's
\texttt{+RTS -s} flags, below is an exerpt of the output:

\begin{verbatim}
  SPARKS: 12 (12 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    7.27s  (  7.28s elapsed)
  GC    time    0.61s  (  0.72s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time    7.88s  (  8.00s elapsed)
\end{verbatim}

We can see that there are only 12 sparks generated by this program: in
fact the program creates one spark per word in the search list, of
which there are 12.  This rather coarse granularity will certainly
limit the ability of the runtime to effectively load-balance as we
increase the number of cores, but that won't be an issue with a small
number of cores.

Initially we try with 4 cores, and using our best parallel GC settings
(see \cite{multicore-ghc}):

\begin{verbatim}
  SPARKS: 12 (11 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    8.15s  (  2.21s elapsed)
  GC    time    4.50s  (  1.17s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   12.65s  (  3.38s elapsed)
\end{verbatim}

Not bad: 8.00/3.38 is a speedup of around 2.4 on 4 cores.  But since
this program has a highly parallel structure, we might hope to do
better.  

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{soda1.png}
\end{center}
\caption{Soda ThreadScope profile}
\label{f:soda-threadscope}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{soda2.png}
\end{center}
\caption{Soda ThreadScope profile (zoomed initial portion)}
\label{f:soda-threadscope2}
\end{figure*}

Figure~\ref{f:soda-threadscope} shows the ThreadScope profile for this
version of soda.  We can see that while an overall view of the runtime
shows a reasonable parallelisation, if we zoom into the initial part
of the run (Figure~\ref{f:soda-threadscope2}) we can see that HEC 0 is
running continuously, but threads on the other HECs are running very
briefly and then immediately getting blocked.

Going back to the program, we can see that the grid of letters is
generated lazily by a function \codef{mk\_grid}.  What is happening here is
that the main thread creates sparks before the grid has been
evaluated, and then proceeds to evaluate the grid.  As each spark
runs, it blocks almost immediately waiting for the main thread to
complete evaluation of the grid.

This type of blocking is often not disastrous, since a thread will become
unblocked soon after the thunk on which it is blocking is evaluated
(see the discussion of ``blackholes'' in \citet{multicore-ghc}).  There
is nevertheless a short delay between the thread becoming runnable
again and the runtime noticing this and moving the tread to the run
queue.  Sometimes this delay can be hidden if the program has other
sparks it can be running in the meantime, but that is not the case
here.  There are also costs associated blocking the thread and waking
it up again, which we would like to avoid if possible.

One way to avoid this is to evaluate the whole grid before creating
any sparks.  This is achieved by adding a call to \codef{rnf}:

\begin{lstlisting}
        -- force the grid to be evaluated:
        evaluate (rnf grid)
\end{lstlisting}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{soda3.png}
\end{center}
\caption{Soda ThreadScope profile (evaluating the input grid eagerly)}
\label{f:soda-threadscope3}
\end{figure*}

The effect on the profile is fairly dramatic
(Figure~\ref{f:soda-threadscope3}).  We can see that the parallel
execution doesn't begin until around 500ms into the execution:
creating the grid is taking quite a while.  The program also runs
slightly faster in parallel now:

\begin{verbatim}
  SPARKS: 12 (11 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    7.62s  (  2.31s elapsed)
  GC    time    3.35s  (  0.86s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   10.97s  (  3.18s elapsed)
\end{verbatim}
which we attribute to less blocking and unblocking of threads.  We can
also see that this program now has a significant sequential section -
around 15\% of the execution time - which limits the maximum speedup
we can achieve with 4 cores to 2.7, and we are already very close to
that at 2.5.

To improve parallelism further with this example we would have to
parallelise the creation of the initial grid; this probably isn't
hard, but it would be venturing beyond the realms of realism somewhat
to optimse the creation of the input data for a synthetic benchmark,
so we conclude the case study here.  It has been instructional to see
how thread blocking appears in the ThreadScope profile, and how to
avoid it by pre-evaluating data that is needed on multiple CPUs.

Here are a couple more factors that may be affecting the speedup we
see in this example:

\begin{itemize}
\item The static grid data is created on one CPU and has to be fecthed
  into the caches of the other CPUs.  We hope in the future to be able
  to show the rate of cache misses (and similar characteristics) on
  each CPU alongside the other information in the ThreadScope profile,
  which would highlight issues such as this.
\item The granularity is too large: we can see that the HECs finish
  unevenly, losing a little parallelism at the end of the run.
\end{itemize}

\subsection{minimax}

Minimax is another historical Parallel Haskell program.  It is based
on an implementation of alpha-beta searching for the game tic-tac-toe,
from Hughes' influential paper ``Why Functional Programming Matters''
\cite{why}.  For the purposes of this paper we have generalised the
program to use a game board of arbitrary size: the original program
used a fixed 3x3 grid, which is too quickly solved to be a useful
parallelism benchmark nowadays.  However $4x4$ still represents a
sufficient challenge without optimising the program further.

For the examples that follow, the benchmark is to evaluate the game
tree 6 moves ahead, on a 4x4 grid in which the first 4 moves have
already been randomly played.

We will explore a few different parallelisations of this program using
ThreadScope.  The function for calculating the best line in the game
is \codef{alternate}:

\begin{lstlisting}
alternate depth player f g board
 = move : alternate depth opponent g f board'
 where
   move@(board',_) = best f possibles scores
   scores          = map (bestMove depth opponent g f) possibles
   possibles       = newPositions player board
   opponent        = opposite player
\end{lstlisting}

This function calculates the sequence of moves in the game that give
the best outcome (as calculated by the alpha-beta search) for each
player.  At each stage, we generate the list of possible moves
(\codef{newPositions}), evaluate each move by alpha-beta search on the
game tree (\codef{bestMove}), and pick the best one (\codef{best}).

Let's run the program sequentially first to establish the baseline
runtime:

\begin{verbatim}
  14,484,898,888 bytes allocated in the heap

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    8.44s  (  8.49s elapsed)
  GC    time    3.49s  (  3.51s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   11.94s  ( 12.00s elapsed)
\end{verbatim}

One obvious way to parallelise this problem is to evaluate each of the
possible moves in parallel.  This is easy to achieve with a
\codef{parListWHNF} strategy:

\begin{lstlisting}
  scores = map (bestMove depth opponent g f) possibles
             `using` parListWHNF
\end{lstlisting}

And indeed this does yield a reasonable speedup:

\begin{verbatim}
  14,485,148,912 bytes allocated in the heap

  SPARKS: 12 (11 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    9.19s  (  2.76s elapsed)
  GC    time    7.01s  (  1.75s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   16.20s  (  4.52s elapsed)
\end{verbatim}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{minimax1.png}
\end{center}
\caption{Minimax ThreadScope profile}
\label{f:minimax-threadscope1}
\end{figure*}

A speedup of 2.7 on 4 processors is a good start!  However, looking at
the ThreadScope profile (Figure~\ref{f:minimax-threadscope1}), we can
see that there's a jagged edge on the right: our granularity is too
large, and we don't have enough work to keep all the processors busy
until the end.  What's more, as we can see from the runtime
statistics, there were only 12 sparks, corresponding to the 12
possible moves in the 4x4 grid after 4 moves have already been played.
In order to scale to more CPUs we will need to find more parallelism.

The game tree evaluation is defined as follows:

\begin{lstlisting}
bestMove :: Int -> Piece -> Player -> Player -> Board
         -> Evaluation
bestMove depth p f g 
  = mise f g 
  . cropTree
  . mapTree static
  . prune depth
  . searchTree p
\end{lstlisting}

Where \codef{searchTree} lazily generates a search tree starting
from the current position, with player \texttt{p} to play next.  The
function \codef{prune} prunes the search tree to the given depth, and
\codef{mapTree static} applies a static evaluation function to each
node in the tree.  The function \codef{cropTree} prunes branches below
a node in which the game has been won by either player.  Finally,
\codef{mise} performs the alpha-beta search, where \codef{f} and
\codef{g} are the min and max functions over evaluations for the
current player \codef{p}.

We must be careful with parallelisation here, because the algorithm is
relying heavily on lazy evaluation to avoid evaluating parts of the
game tree.  Certainly we don't want to evaluate beyond the prune
depth, and we also don't want to evaluate beyond a node in which one
player has won (\codef{cropTree} prunes further moves after a win).
The alpha-beta search will prune even more of the tree: for example,
there is no point evaluating further down a branch if it has already
been established that there is a winning move.  So unless we are
careful, some of the parallelism we add here may be wasted
speculation.

The right place to parallelise is in the alpha-beta search itself.
Here is the sequential code:

\begin{lstlisting}
mise :: Player -> Player -> Tree Evaluation -> Evaluation
mise f g (Branch a []) = a
mise f g (Branch _ l) = foldr f (g OWin XWin) (map (mise g f) l)
\end{lstlisting}

The first equation looks for a leaf, and returns the evaluation of the
board at that point.  A leaf is either a completed game (either drawn
or a winning position for one player), or the result of pruning the
search tree.  The second equation is the interesting one: \codef{foldr
  f} picks the best option for the current player from the list of
evaluations at the next level.  The next level evaluations are given
by \codef{map (mise g f) l}, which picks the best options for the
\emph{other} player (which is why the \codef{f} and \codef{g} are
reversed).

The \codef{map} here is a good opportunity for parallelism.  Adding
a \codef{parListWHNF} strategy should be enough:

\begin{lstlisting}
mise f g (Branch _ l) = foldr f (g OWin XWin) 
                         (map (mise g f) l `using` parListWHNF)
\end{lstlisting}
However, this will try to parallelise every level of the search,
leading to some sparks with very fine granularity.  Also it may
introduce too much speculation: elements in each list after a win do
not need to be evaluated.  Indeed if we try this we get:

\begin{verbatim}
  22,697,543,448 bytes allocated in the heap

  SPARKS: 4483767 (639031 converted, 3457369 pruned)

  INIT  time    0.00s  (  0.01s elapsed)
  MUT   time   16.19s  (  4.13s elapsed)
  GC    time   27.21s  (  6.82s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   43.41s  ( 10.95s elapsed)
\end{verbatim}

We ran a lot of sparks (600k), but the program took longer to run.
One clue that we are actually speculating useless work is the amount
of allocation.  In the sequential run the runtime reported 14GB
allocated, but this parallel version allocated 22GB\footnote{CPU time
  is not a good measure of speculative work, because in the parallel
  runtime threads can sometimes be spinning while waiting for work,
  particularly in the GC.}.

In order to eliminate some of the smaller sparks, we can
parallelise the alpha-beta to a fixed depth.  This is done by
introducing a new variant of \codef{mise}, \codef{parMise}, that
applies the \codef{parListWHNF} strategy up to a certain depth, and then
calls the sequential \codef{mise} beyond that.  Just using a depth of
one gives quite good results:

\begin{verbatim}
  SPARKS: 132 (120 converted, 12 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    8.82s  (  2.59s elapsed)
  GC    time    6.65s  (  1.70s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   15.46s  (  4.30s elapsed)
\end{verbatim}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{minimax2.png}
\end{center}
\caption{Minimax ThreadScope profile (with parMise 1)}
\label{f:minimax-threadscope2}
\end{figure*}

Though as we can see from the ThreadScope profile
(Figure~\ref{f:minimax-threadscope2}), there are some gaps.
Increasing the threshold to two works nicely:

\begin{verbatim}
  SPARKS: 1452 (405 converted, 1046 pruned)

  INIT  time    0.00s  (  0.03s elapsed)
  MUT   time    8.86s  (  2.31s elapsed)
  GC    time    6.32s  (  1.57s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   15.19s  (  3.91s elapsed)
\end{verbatim}

\begin{figure*}
\begin{center}
\includegraphics[scale=0.3]{minimax3.png}
\end{center}
\caption{Minimax ThreadScope profile (with parMise 2)}
\label{f:minimax-threadscope3}
\end{figure*}

We have now achieved a speedup of 3.1 on 4 cores against the
sequential code, and as we can see from the final ThreadScope profile
(Figure~\ref{f:minimax-threadscope3}) all our cores are kept busy.

We found that increasing the threshold to 3 starts to cause
speculation of unnecessary work.  In 4x4 tic-tac-toe most positions
are a draw, so it turns out that there is little speculation in the
upper levels of the alpha-beta search, but as we get deeper in the
tree, we find positions that are a certain win for one player or
another, which leads to speculative work if we evaluate all the moves
in parallel.  

Ideally GHC would have better support for speculation: right now,
speculative sparks are not garbage collected when they are found to be
unreachable.  We do plan to improve this in the future, but
unfortunately changing the GC policy for sparks is incompatible with
the current formulation of Strategies \cite{multicore-ghc}.

\input{related-work}

\section{Conclusions and Further work}
\label{s:conclusion}
We have shown how thread-based profile information can be effectively
used to help understand and fix parallel performance bugs in 
semi-explicitly parallel Haskell programs. Many of the techniques
we present in this paper are also applicable to other kinds of
concurrent and parallel Haskell programs e.g. programs that
explicitly create threads through \codef{forkIO} and synchronize
through \codef{MVars} as well as programs that use atomic blocks
implemented by software transactional memory. Furthermore, we expect
the profiling techniques in this paper to be of great benefit to
developers using Data Parallel Haskell. 

The ability to profile parallel Haskell programs plays an important
part in the development of such programs because the analysis
process motivates the need to develop specialized stratagies to
help control evaluation order, extent and granularity as we demonstrated in
the minmax example.

Here are some of the futurue directions we would like to take this
work:

\begin{itemize}
\item Improve the user interface and navigation of ThreadScope.  For
  example, it would be nice to filter the display to show just a
  subset of the threads, in order to focus on the behaviour of a
  particular thread or group of threads.

\item It would also be useful to understand how threads interact with each 
   other via \codef{MVars} e.g. to make it easier to see which 
   threads are blocked on read and write accesses to \codef{MVar}s.

\item The programmer should be able to generate events
  programmatically, in order to mark positions in the timeline so that
  different parts of the program's execution can easily be identified
  and separated in ThreadScope.

\item It would be straighforward to produce graphs similar to those
  from the GpH programming tools \cite{gph}.

\item Combine the timeline profile with information from the OS and
  CPU.  For example, for IO-bound concurrent programs we might like to
  see IO or network activity displayed on the timeline.  Information
  from CPU performance counters could also be superimposed or
  displayed alongside the thread timelines, providing insight into
  cache behaviour, for example.

\item Have the runtime system generate more tracing information, so
  that ThreadScope can display information about such things as memory
  usage, run queue sizes, spark pool sizes, and foreign call activity.
\end{itemize}

The authors would like to acknowledge the work of the developers
of previous Haskell concurrent and parallel profiling systems
which have provided much inspiration for our own work. Specifically
work on GpH, GranSim and Eden was particularly useful.

{\small
\bibliographystyle{plainnat}
\bibliography{ghc-parallel-tuning}
}

\end{document}
