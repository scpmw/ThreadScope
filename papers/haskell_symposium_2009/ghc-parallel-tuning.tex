\documentclass[twocolumn,9pt]{sigplanconf}

\usepackage{url}
% \usepackage{code}
\usepackage{graphicx}
\usepackage{enumerate}

\usepackage{listings}
\lstset{basicstyle=\fontfamily{cmss} \small, columns=fullflexible, language=Haskell, numbers=left, numberstyle=\tiny, numbersep=2pt}

\newcommand{\codef}[1]{{\fontfamily{cmss}\small#1}}
\newcommand{\boldcode}[1]{{\bf\fontfamily{cmss}\small#1}}


\nocaptionrule

\title{Parallel Performance Tuning for Haskell}

\authorinfo{Donnie Jones}{University of Kentucky}
           {donnie@darthik.com}
\authorinfo{Simon Marlow}{Microsoft Research}
           {simonmar@microsoft.com}
\authorinfo{Satnam Singh}{Microsoft Research}
           {satnams@microsoft.com}

\begin{document}

\maketitle
%\makeatactive

\begin{abstract}
Profiling information is essential for the performance tuning of parallel
programs. This paper describes a new parallel profiling mechansim in the Glasgow Haskell Compiler (GHC) run-time system and its associated graphical viewer. We illustrate how this system can be used to help diagnose and fix several common kinds of performance errors in semi-explicit parallel Haskell programs.
\end{abstract}


\section{Introduction}
Semi-explicit parallel programs in Haskell are written using annotations to control the granularity of parallelsim and the squenecing of evaluation~\cite{spj:trin98b}. This leads to deterministic fine-grain parallel programs that can be executed on multi-core computers. However, it can be difficult to observe how effectively a program has been parallelized. This paper presents some recently developed profiling technology in the GHC run-time system and its associated graphical viewer which can be used for parallel performance tuning of Haskell programs.

\section{Background}

\section{Profiling Motivation}
Show examples of semi-explicit parallel programs that go wrong. Show what we could measure before using heap and time profiling and motivate the need for better profiling.

Haskell provides a mechanism to allow the user to control the granularity of parallelism by indicating what computations may be usefully carried out in parallel. This is done by using functions from the \codef{Control.Parallel} module. The interface for \codef{Control.Parallel} is shown below:
\begin{lstlisting}
  par :: a -> b -> b 
  pseq :: a -> b -> b 
\end{lstlisting}
The function \codef{par} indicates to the GHC run-time system that it may be beneficial to evaluate the first argument in parallel with the second argument. The \codef{par} function returns as its result the value of the second argument. One can always eliminate \codef{par} from a program by using the following identity without altering the semantics of the program:
\begin{lstlisting}
  par a b = b 
\end{lstlisting}
A thread is not necessarily created to compute the value of the expression \codef{a}. Instead, the GHC run-time system creates a {\em spark} which has the potential to be executed on a different thread from the parent thread. A sparked computation expresses the possibility of performing some speculative evaluation. Since a thread is not necessarily created to compute the value of \codef{a} this approach has some similarities with the notion of a {\em lazy future}~\cite{mohr:91}.

Sometimes it is convenient to write a function with two arguments as an infix function and this is done in Haskell by writing quotes around the function:
\begin{lstlisting}
  a `par` b
\end{lstlisting}

We call such programs semi-explicitly parallel because the programmer has provided a hint about the appropriate level of granularity for parallel operations and the system implicitly creates threads to implement the concurrency. The user does not need to explicitly create any threads or write any code for inter-thread communication or synchronization.

To illustrate the use of \codef{par} we present a program that performs two compute intensive functions in parallel. The first compute intensive function we use is the notorious Fibonacci function:
\begin{lstlisting}
fib :: Int -> Int
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting}
The second compute intensive function we use is the \codef{sumEuler} function taken from~\cite{trinder:02}:
\begin{lstlisting}
mkList :: Int -> [Int]
mkList n = [1..n-1]

relprime :: Int -> Int -> Bool
relprime x y = gcd x y == 1

euler :: Int -> Int
euler n = length (filter (relprime n) (mkList n))

sumEuler :: Int -> Int
sumEuler = sum . (map euler) . mkList
\end{lstlisting}
The function that we wish to parallelize adds the results of calling \codef{fib} and \codef{sumEuler}:
\begin{lstlisting}
sumFibEuler :: Int -> Int -> Int
sumFibEuler a b = fib a + sumEuler b
\end{lstlisting}
As a first attempt we can try to use \codef{par} the speculatively spark off the computation of \codef{fib} while the parent thread works on \codef{sumEuler}:
\begin{lstlisting}
parSumFibEuler :: Int -> Int -> Int
parSumFibEuler a b
  = f `par` (f + e)
    where
    f = fib a
    e = sumEuler b
\end{lstlisting}

\section{Profiling Infrastructure}
The basic infrastrucutre of the systsem, design choices and justification and a little about the viewer ThreadScope.

\subsection{GHC-Events}
GHC-Events is a general purpose event-logging framework with two components: GHC-EventLog, GHC-Events library.  GHC-EventLog is the event logger in the GHC run-time system, which was designed with a focus on speed and extensibility while incurring minimal performance overhead for Haskell programs yet achieving detailed profiling information.  The GHC-Events library interprets the GHC-EventLog binary log file to provide an interface to the events for any visualizer.

Minimizing the performance overhead is critical when designing a system to facilitate profiling such as GHC-Events.  A pre-allocated buffer is created by GHC-Events for each GHC run-time system Capability to store its events, by doing so we prevent delays from dynamic memory allocation and require no need for locks since the buffers are separated by the Capability.  

New events can easily be added to GHC-EventLog simply by defining the new event in the GHC-EventLog header file, EventLogFormat.h, with the subsequent integer value as the event identifier.  This GHC-EventLog header is shared between GHC-EventLog and GHC-Events library since it holds the definition of each event.  To avoid identifier conflicts and ensure backwards-compatibility for the GHC-Events framework, event identifiers are not re-used, but rather events are deprecated and replaced by new events.  

Usage of GHC-EventLog is quite straightforward: create (or use) an event from the GHC-EventLog header, call the GHC-EventLog post-event function with the event identifier from the GHC run-time system.  To solidify understanding of the usage of GHC-EventLog, below is the C source code added to GHC-EventLog and the GHC run-time system for the essential parallel profiling event: create thread. TODO!

The output from GHC-EventLog is generated as a binary log file.  The GHC-Events library is the interface to interpret the binary log file for any visualizer, e.g. the ThreadScope visualizer relies upon the GHC-Events library to parse the binary file and make available data-types containing the events.

\subsection{ThreadScope}

\section{Case Studies}
Revist examples and show how performance can be tuned with the information obtained from the profiler.

\subsection{SumEuler}
This example has been used several times before and it is useful for illustrating how sparks should be carefully constructed and the need for pseq.

\subsection{Soda}

Soda is a program for solving word-search problems: given a
rectangular grid of letters, find occurrences of a words from a
supplied list, where a word can appear horizonally, vertically, or
diagnoally, in either direction (giving a total of eight possible
orientations).

The program has a long history as a Parallel Haskell benchmark
\cite{soda}.  The version we start with here is a recent incarnation,
using a random initial grid with a tunable size.  The words do not in
fact appear in the grid; the program just fruitlessly searches the
entire grid for a predefined list of words.  One advantage of this
formulation for benchmark purposes is that the program's performance
does not depend on the search order, however a disadvantage is that
the parallel structure is unrealistically regular.

The parallelism is expressed using \codef{parListWHNF} \cite{multicore-ghc}
to avoid the space leak issues with the standard strategy
implementation of \codef{parList}.

To establish the baseline performance, we run the program using GHC's
\texttt{+RTS -s} flags, below is an exerpt of the output:

\begin{verbatim}
  SPARKS: 12 (12 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    7.27s  (  7.28s elapsed)
  GC    time    0.61s  (  0.72s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time    7.88s  (  8.00s elapsed)
\end{verbatim}

We can see that there are only 12 sparks generated by this program: in
fact the program creates one spark per word in the search list, of
which there are 12.  This rather coarse granularity will certainly
limit the ability of the runtime to effectively load-balance as we
increase the number of cores, but that won't be an issue with a small
number of cores.

Initially we try with 4 cores, and using our best parallel GC settings
(see \cite{multicore-ghc}):

\begin{verbatim}
  SPARKS: 12 (11 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    8.15s  (  2.21s elapsed)
  GC    time    4.50s  (  1.17s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   12.65s  (  3.38s elapsed)
\end{verbatim}

Not bad: 8.00/3.38 is a speedup of around 2.4 on 4 cores.  But since
this program has a highly parallel structure, we might hope to do
better.  

Figure~\ref{f:soda-threadscope} shows the ThreadScope profile for this
version of soda.  We can see that while an overall view of the runtime
shows a reasonable parallelisation, if we zoom into the initial part
of the run (Figure~\ref{f:soda-threadscope2}) we can see that HEC 0 is
running continuously, but threads on the other HECs are running very
briefly and then immediately getting blocked.

Going back to the program, we can see that the grid of letters is
generated lazily by a function \codef{mk\_grid}.  What is happening here is
that the main thread creates sparks before the grid has been
evaluated, and then proceeds to evaluate the grid.  As each spark
runs, it blocks almost immediately waiting for the main thread to
complete evaluating the grid.

This type of blocking is often not disastrous, a thread will become
unblocked soon after the thunk on which it is blocking is evaluated
(see the discussion of ``blackholes'' in \cite{multicore-ghc}).  There
is nevertheless a short delay between the thread becoming runnable
again and the runtime noticing this and moving the tread to the run
queue.  Sometimes this delay can be hidden if the program has other
sparks it can be running in the meantime, but that is not the case
here.  There are also costs associated blocking the thread and waking
it up again, which we would like to avoid if possible.

One way to avoid this is to evaluate the whole grid before creating
any sparks.  This is achieved by adding a call to \codef{rnf}:

\begin{lstlisting}
        -- force the grid to be evaluated:
        evaluate (rnf grid)
\end{lstlisting}

The effect on the profile is fairly dramatic
(Figure~\ref{f:soda-threadscope3}).  We can see that the parallel
execution doesn't begin until around 500ms into the execution:
creating the grid is taking quite a while.  The program also runs
slightly faster in parallel now:

\begin{verbatim}
  SPARKS: 12 (11 converted, 0 pruned)

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    7.62s  (  2.31s elapsed)
  GC    time    3.35s  (  0.86s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   10.97s  (  3.18s elapsed)
\end{verbatim}
which we attribute to less blocking and unblocking of threads.  We can
also see that this program now has a significant sequential section -
around 15\% of the execution time - which limits the maximum speedup
we can achieve with 4 cores to 2.7, and we are already very close to
that at 2.5.

To improve parallelism further with this example we would have to
parallelise the creation of the initial grid; this probably isn't
hard, but it would be venturing beyond the realms of realism somewhat
to optimse the creation of the input data for a synthetic benchmark,
so we conclude the case study here.  It has been instructional to see
how thread blocking appears in the ThreadScope profile, and how to
avoid it by pre-evaluating data that is needed on multiple CPUs.

Here are a couple more factors that may be affecting the speedup we
see in this example:

\begin{itemize}
\item The static grid data is created on one CPU and has to be fecthed
  into the caches of the other CPUs.  We hope in the future to be able
  to show the rate of cache misses (and similar characteristics) on
  each CPU alongside the other information in the ThreadScope profile,
  which would highlight issues such as this.
\item The granularity is too large: we can see that the HECs finish
  unevenly, losing a little parallelism at the end of the run.
\end{itemize}

\begin{figure}
\caption{Soda ThreadScope profile}
\label{f:soda-threadscope}
\end{figure}

\begin{figure}
\caption{Soda ThreadScope profile (zoomed initial portion)}
\label{f:soda-threadscope2}
\end{figure}

\begin{figure}
\caption{Soda ThreadScope profile (evaluating the input grid eagerly)}
\label{f:soda-threadscope3}
\end{figure}

\subsection{minimax}

Minimax is another historical Parallel Haskell program.  It is based
on an implementation of alpha-beta searching for the game tic-tac-toe,
from Hughes' influential paper ``Why Functional Programming Matters''
\cite{why}.  For the purposes of this paper we have generalised the
program to use a game board of arbitrary size: the original program
used a fixed $3x3$ grid, which is too quickly solved to be a useful
parallelism benchmark nowadays.  However $4x4$ still represents a
sufficient challenge without optimising the program further.

\input{related-work}

\section{Conclusions}

\bibliographystyle{alpha}
\bibliography{ghc-parallel-tuning}

\end{document}
