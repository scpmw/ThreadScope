\documentclass[twocolumn,9pt]{sigplanconf}

% ToDo
%   - abstract
%   - add heartbeat event and check the 8-core execution
%   - measure sizes of global BH pool, and amount of BH blocking
%   - related work (Satnam)
%   - table showing all measurements together?
%   - comment on why don't we get full speedup
%   - spellcheck
%   - do any ICFP-specific formatting (e.g. is the bibliography right?)

\usepackage{url}
\usepackage{code}
% \usepackage{comment}
\usepackage{graphicx}
\usepackage{enumerate}

\newcommand{\ToDo}[1]{\par{{\bf ToDo:} \sl #1}\par}
\newcommand{\sdm}[1]{{{\bf SDM:} \sl #1}}
\newcommand{\spj}[1]{{{\bf SLPJ:} \sl #1}}
% \newcommand{\ss}[1]{{{\bf SS:} \sl #1}}

\nocaptionrule

\title{Implicit Parallelism in Multicore Haskell}

\authorinfo{Simon Marlow}{Microsoft Research}
           {simonmar@microsoft.com}
\authorinfo{Simon Peyton Jones}{Microsoft Research}
           {simonpj@microsoft.com}
\authorinfo{Satnam Singh}{Microsoft Research}
           {satnams@microsoft.com}

\begin{document}

\maketitle
\makeatactive

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{s:intro}

At least in theory, Haskell has a head start in the race to find an
effective way to program parallel hardware.  Purity-by-default means
that there should be a wealth of inherent parallelism in Haskell code,
and the ubiquitous lazy evaluation model means that, in a sense,
\emph{futures} are built-in.

How can we turn these benefits into real speedups on commodity
hardware?  This paper documents our experiences with building and
optimising a parallel runtime for Haskell.  Our runtime supports three
models of parallelism: explicit thread-based concurrency
\cite{jones96concurrent}, semi-implicit deterministic parallelism
\cite{trinder:strategies}, and data-parallelism \cite{spj:harnessing}.
In this paper, however, we
focus entirely on semi-implicit parallelism.

Completely implicit parallelism is still a distant goal; one recent
attempt at this in the context of Haskell can be found in
\cite{fdip}.  The semi-implicit GpH programming model, in
contrast, has been shown to be remarkably effective \cite{cpe, loidl:comparing}.  The
semantics of the program remains 
completely deterministic, and the programmer is not required
to identify threads, communication, or synchronisation.  They merely
annotate sub-computations that might be evaluated in parallel, leaving
the choice of whether to actually do so to the runtime system.
So-called \emph{sparks} can be very fine-grained, and have a lot in
common with \emph{futures} \cite{} \sdm{todo}.

Our goal is that programmers should be able to take existing Haskell
programs, and with a little high-level knowledge of how the program
should parallelise, make some small modifications to the program using
existing well-known techniques, and thereby achieve decent speedup on
today's parallel hardware.  However, when we started benchmarking some
existing Parallel Haskell programs, we found that many programs which
at first glance appeared to be completely reasonable-looking parallel
programs, in fact failed to achieve significant speedup when run with
our implementation on parallel hardware.

By developing some profiling and tracing tools
(Section~\ref{s:viewer}), we were able to identify some of these
bottlenecks and tune the execution model and runtime system.  In this
paper we present our results for applying a variety of optimisations,
that in some cases make dramatic improvements over the baseline
parallel execution speed with GHC 6.10.1.

The impact of these improvements is to make parallel execution more
accessible to the Haskell programmer: having removed some of the
bottlenecks, more programs will parallelise with less effort than
before.  Furthermore, our profiling tools will enable the programmer
to identify bottlenecks and modify their program to work around them.

The contributions of this paper compared to earlier work are as
follows:

\begin{itemize}
\item We identify and discuss two subtle problems with the
  well-established GpH programming model, and give suggestions for
  ways to avoid the pitfalls.

\item Previous work on shared-memory parallel execution of Haskell has
  focused on key individual ideas, for example the method for
  avoiding atomic operations in the evaluation model
  \cite{multiproc05}.  In this paper we describe the complete
  architecture of our runtime system.  Building a
  runtime system designed for parallelism is one thing; doing so
  without compromising on sequential execution performance and at the
  same time implementing the full range of functionality necessary for
  an industrial-strength programming language (e.g. our foreign
  function interface \cite{concffi04}), is another matter.

\item We identify a number of design decisions in the runtime system
  and execution model that affect parallel execution, and perform a
  series of rigorous measurements to evaluate them.  While some of the
  decisions seem obvious, in practice the results are sometimes
  surprising.  Perhaps the most interesting results to emerge are
  related to parallel GC: we found that it was important \emph{not} to
  do load-balancing in the parallel GC, because this spoils locality,
  sometimes with dramatic results.

\end{itemize}

Some of the improvements were described in earlier work
\cite{berthold:comparing}, along with preliminary measurements, as
part of a comparison between shared-heap and distributed-heap
parallel execution models.  In this paper we extend both the range of
measurements and the range of improvements, while focussing
exclusively on shared-heap execution.

It should be stressed that all our results are, or will be, repeatable
using a released version of the widely-used GHC compiler.  Our results
do not require special builds of the compiler or libraries: identical
results will be obtainable using a standard binary distribution of
GHC.  At the time of writing, most of our developments have been made
public in the GHC source code repository, and we expect to include the
remaining changes in the forthcoming 6.12.1 release of GHC, scheduled
for the autumn of 2009.  The sources to our benchmark programs are
available in the public @nofib@ source repository.

% Putting the paper in context:
% 
% \begin{itemize}
% \item We showed how to implement shared-memory parallelism in
%    \cite{multiproc05}
% 
% \item We showed how to implement multi-threaded FFI in
%    \cite{concffi04}
% 
% \item We gave a design for a higher-level scheduler in
%   \cite{concsubstrate07}.
%
% \end{itemize}

\section{Background: programming model}
\label{s:prog-model}

The basic programming model is known as Glasgow Parallel Haskell, or
GpH \cite{trinder:strategies}, and consists of two combinators:

\begin{verbatim}
  par  :: a -> b -> b
  pseq :: a -> b -> b
\end{verbatim}
The semantics of @par a b@ is simply the value of @b@, whereas the
semantics of @pseq@ is given by 

\begin{tabular}{lll}
\\
@pseq a b@&$= \bot$, &\mbox{if}~@a = @$\bot$ \\
          &$= $@ b@, &\mbox{otherwise} \\
\\
\end{tabular}

\noindent
Informally, @par@ stores its first argument as a \emph{spark} in the \emph{spark pool}, 
and then continues by evaluating its second argument.  The intention is that
idle processors can find (probably) useful work in the spark pool.
Typically the first
argument to @par@ will be an expression that is shared by another part
of the program, or will be an expression that refers to other such
shared expressions.

The @pseq@ combinator is used for sequencing; informally, it evaluates
its first argument to weak-head normal form, and then evaluates its
second argument, returning the value of its second argument.  
We discuss in Section~\ref{s:pseq} how it differs from Haskell's @seq@.

\paragraph{Fizzled sparks.}

By the time the spark is removed from the spark pool, it may have been already
evaluated --- we say that it has \emph{fizzled}.  For example,
consider\footnote{We use parentheses here to avoid confusion, but
  since both @`par`@ and @`pseq`@ associate to the right and have
  lower precedence than @+@, we could leave out the parentheses.}
$$@x `par` (y `pseq` (x+y))@$$
This sparks the thunk @x@ (adding it to the spark pool), 
evaluates @y@, and then adds @x@ and @y@.  The addition
operation forces both its arguments, so if the sparked thunk 
@x@ has not been taken up by some other processor,
the addition will evaluate it.  In that case, the spark
has fizzled.  

Indeed, we expect most sparks to fizzle.  
The @par@ operation creates \emph{opportunities} for
parallel evaluation but, if the machine is busy, few of these opportunities
are taken up.  So we must try hard to ensure that:
\begin{itemize}
\item Sparking a thunk is cheap. 
\item Fizzled sparks do not clog up the spark pool.
\end{itemize}

\paragraph{Strategies.}
In \emph{Algorithms + Strategies = Parallelism} \cite{trinder:strategies},
Trinder \emph{et al} explain how to use so-called ``strategies'' to modularise
the construction of parallel programs.  In brief, the idea is as follows.
A strategy is a a function that may evaluate (parts of) its argument
and create sparks, but has no interesting results:
\par{\small
\begin{code}
  type Done = ()
  done = ()
  type Strategy a = a -> Done
\end{code}
}
Strategies compose nicely; that is, we can build complex strategies out of simpler ones:
\par{\small
\begin{verbatim}
  rwhnf :: Strategy a
  rwhnf x = x `pseq` done

  parList :: Strategy a -> Strategy [a]
  parList strat []     = done
  parList strat (x:xs) = strat x `par` parList strat xs
\end{verbatim}
}
Finally, we can combine a data structure with a strategy for evaluating it in 
parallel:
\par{\small
\begin{code}
  using :: a -> Strategy a -> a 
  using x s = s x `pseq` x
\end{code}
}
Here is how we might use the combinators to evaluate all the element of a (lazy) 
input list in parallel, and then add them up:
\par{\small
\begin{code}
  psum :: [Int] -> Int
  psum xs = sum xs `using` parList rwhnf
\end{code}
}
\section{The need for \texttt{pseq}} \label{s:pseq}

Previous work \cite{trinder:strategies} uses Haskell's existing @seq@ combinator,
where Section~\ref{s:prog-model} uses ``@pseq@''.
The two have an identical denotational meaning, but differ in their
operational behaviour.  We consider @seq@ to be strict in \emph{both} of
its arguments, since both @seq a @$\bot$@ = @$\bot$ and @seq @$\bot$
a@ = @$\bot$.  This means that a compiler can reasonably perform the
evaluation of the arguments to @seq@ in any order, according to
whichever is more beneficial (or even, evaluate them in parallel).

However, in the context of parallel programming we need an operator
for sequencing evaluation with respect to @par@.  For example, here is
a definition of parallel map, @parMap@:

\begin{verbatim}
parMap f []     = []
parMap f (x:xs) = y `par` (ys `pseq` y:ys)
   where y  = f x
         ys = parMap f xs
\end{verbatim}
The intention here is to spark the evaluation of @f x@, and then
evaluate @parMap f xs@, before returning the new list @y:ys@.  The
programmer is hoping to express an \emph{ordering} of the evaluation:
\emph{first} spark @y@, \emph{then} evaluate @ys@.  Such ordering
constraints cannot normally be expressed in Haskell: Haskell specifies
denotational behaviour only, leaving evaluation strategy completely in
the hands of the implementation.

The key point here is that Haskell's @seq@ is not suitable for
expressing the ordering constraint that we require.  We could enforce an
implementation-specific constraint on the behaviour of @seq@, imbuing
it with an ordering property.  However, @seq@ is pervasive in
sequential Haskell programs today, either explicitly or implicitly as
a result of strictness annotations on data types and function
arguments, and to require @seq@ to evaluate its first argument before
(any part of) its second argument would unnecessarily restrict the
available transformations and optimisations that the compiler can
perform.

% SLPJ: Repetition
% Hence, we introduce the @pseq@ combinator, with identical denotational
% meaning to @seq@, but with an additional constraint on its operational
% behaviour that its first argument is evaluated before its second.  

To our knowledge this is the first time that this subtle point has
been published; the @pseq@ operator first appeared in GHC 6.8.1.

\section{The space behaviour of \texttt{par}}
\label{s:space-par}

The spark pool should ideally contain only useful work,
and we might hope that the garbage collector would assist the 
scheduler by removing useless sparks from the spark pool.
Clearly a fizzled spark is useless, and the garbage collector
can (and does) discard them, but which \emph{other} 
sparks should the garbage collector retain?  Two policies
immediately spring to mind, that we shall call ROOT and WEAK:

\begin{itemize}
\item ROOT: Treat (non-fizzled) sparks as \emph{roots} for the garbage
  collector. That is, retain all such sparks and the graph they point to.

\item WEAK: Only retain (non-fizzled) sparks that are reachable from the roots of
  the program.
\end{itemize}
\noindent The problem is, neither of these policies is satisfactory.  WEAK seems
attractive, because it lets us discard sparks that are no longer
required by the program.  However, the WEAK policy is completely
incompatible with strategies.  Consider the @parList@ strategy:

{\small
\begin{verbatim}
parList :: Strategy a -> Strategy [a]
parList strat []     = ()
parList strat (x:xs) = strat x `par` parList strat xs
\end{verbatim}
}

Each spark generated by @parList@ is a thunk for the expression
``@strat x@''; this thunk is not shared, since it is created uniquely
for the purposes of creating the spark, \emph{and hence can never fizzle}.  
Hence, the WEAK policy will
discard all sparks created by @parList@, which is obviously
undesirable.

So, what about the ROOT policy?  This is the policy that is used in
existing implementations of GpH, including GUM \cite{Trinder:gum} and
GHC.  However, it leads to the converse problem:
\emph{too many} sparks are retained, leading to space leaks.  Consider
the expression
$$@sum (parList rnf (map expensive [1..100000]))@$$
With the ROOT policy we will retain all
of the sparks created by @parList@, and hence lose no parallelism.
But if there are not enough processor cores to evaluate all of the
sparks, they will never be garbage collected, 
\emph{even after the @sum@ is complete}!
They remain in the spark pool, retaining the list elements that they point
to.  This can lead to serious space leaks\footnote{In fact, this space
  leak was reported to the GHC team as a bug,
  \url{http://hackage.haskell.org/trac/ghc/ticket/2185}.}.

To quantify the effect, we compared two versions of the ray tracing
benchmark that we use for later measurements
(Section~\ref{s:benchmarks}).  The first version uses @parBuffer@ from
the standard strategies library, applied to the @rwhnf@ strategy,
while the second uses a modified version of @parBuffer@ which avoids
the space leak (we will explain how the modified version works in
Section~\ref{s:par-space-leak-workaround}).  We ran both versions of
the program on a single CPU, to illustrate the degenerate case of
having too few CPUs to use the available parallelism.
Figure~\ref{f:par-space-leak} gives the results; MUT is the amount of
``mutator time'' (execution time excluding garbage collection), GC is
the time spent garbage collecting.  We can see that with the
strategies version, no sparks fizzle, and the GC time suffers
considerably as a result\footnote{Why don't all the sparks fizzle in
  the second version?  In fact the runtime does manage to execute a
  few sparks while it is waiting for IO to happen.}.

\begin{figure}
\begin{tabular}{l|rrrrr}
              &  Total    & MUT     & GC      & Total  & Fizzled \\
              &  time(s)  & time(s) & time(s) & sparks & Sparks \\
\hline
Strat.    &   10.7 &  5.1 & 5.7 & 1000000 & 0 \\
No Strat. &    6.4 &  5.2 & 1.2 & 1000000 & 999895\\
\end{tabular}
\caption{Comparison of \texttt{ray} using strategies vs. no strategies}
\label{f:par-space-leak}
\end{figure}

Implementations using the ROOT policy have been around for quite a
long time, and yet the problem has only recently come to light.  Why
is this?  We are not sure, but postulate that the applications that
have been used to benchmark these systems do not suffer unduly from
the space leaks, perhaps because the amount of extra space retained is
small, and there is little or no speculation involved.  If there are
enough CPUs to use all the parallelism, then no space leaks are
observed; the problem comes when we want to write a single program
that works well when run both sequentially and in parallel.

Are there any other policies that we should consider?  Perhaps
we might try to develop a policy along the lines of ``discard sparks
that share no graph with the main program''.  This is clearly an
improvement on the ROOT policy because it lets us discard sparks that
share nothing with the main program.  However, it is quite difficult
to establish whether there is any sharing between the spark and the
main program, since this entails establishing a ``reaches'' property,
where each closure in the graph is marked if it can reach certain
other closures (namely the main program).  This is exactly the
opposite of the property that a garbage collector normally
establishes, namely ``is reachable from'', and is therefore at odds
with the way the garbage collector normally works.  It requires a
completely new traversal, perhaps by reversing all the pointers in the
graph.

Even if we could implement this strategy, it does not completely solve
the problem.  A spark may share data with the main program, but that
is not enough: it has to share \emph{unevaluated} data, and that
unevaluated data must be part of what the spark will evaluate.
Moreover, perhaps we still want to discard sparks that are retaining a
lot of unshared data, but still refer to a small amount of shared
data, on the grounds that the cost of the space leak outweighs the
benefits of any possible parallelism.

It the authors' belief that this problem cannot be solved with the
existing programming model, and we must introduce a new form of
parallelism annotation in which the programmer somehow indicates the
lifetime of the spark.  In what follows we outline some techniques for
avoiding the problem; meanwhile we intend to pursue a more
comprehensive solution to the problem in future work.

\subsection{Improving space behaviour of sparks}

One way to improve the space behaviour of sparks is this:
use the WEAK policy for garbage collection.  This guarantees,
by construction, the the spark pool does not leak any space 
whatsoever.  The cost to the programmer is having to remember that
only un-reachable sparks will be discarded.  Sometimes that is just
what you want (the work is no longer useful), and sometimes it is not
(see @parList@ above).  Hence:
\begin{itemize}
\item We can no longer use the strategies abstraction as it stands,
  because every strategy combinator involves sparking unique,
  unshared, thunks.
\end{itemize}
Our current implementation still uses the ROOT (because we only
recently appreciated how bad this policy really is).  However, if 
the programmer can
ensure that all sparks are \emph{eventually evaluated} by the main
program, then the fizzling mechanism will clear them out of the 
spark pool.  Requiring that every spark is eventually evaluated obviously
means that all non-fizzled sparks are reachable from the main program,
so ROOT and WEAK coincide, thereby eliminating space leaks
even with ROOT.  However, we accrue an additional disadvantage:
\begin{itemize}
\item We cannot do pure speculation.  By definition, speculation is
  work that is not known to be required by the main thread of
  execution, which is incompatible with the idea of ensuring all
  parallel work is shared and eventually evaluated by the main thread.
\end{itemize}

\subsection{Programming with WEAK}
\label{s:par-space-leak-workaround}

Despite these two disadvantages (the second of which is relatively
easy to fix by switching to WEAK), we can still use strategy-like
combinators, but they are no longer compositional.  
In the case of @parList@, if we simply want to evaluate
each element to weak-head-normal-form, we use a specialised version of
@parList@:

\begin{verbatim}
parListWHNF :: Strategy [a]
parListWHNF []     = done
parListWHNF (x:xs) = x `par` parListWHNF xs
\end{verbatim}
Now, as long as the list we pass to @parListWHNF@ is also evaluated by
the main program, the sparks will all be garbage collected as usual.
The rule of thumb is to always put a variable on the left of @par@.

Reducing the granularity with @parListChunk@ is a common technique;
here is the standard implementation in the strategies library:

\begin{verbatim}
parListChunk :: Int -> Strategy a -> Strategy [a]
parListChunk n strat [] = done
parListChunk n strat xs = seqListN n strat xs `par` 
			    parListChunk n strat (drop n xs)
\end{verbatim}
but it seems difficult to accommodate this strategy in our new
restricted setting, since each spark will necessary refer to a
separate chunk of list extracted from the main list.  The way to do it
is to use @parListWHNF@:

\begin{verbatim}
parListChunkWHNF :: Int -> [a] -> [a]
parListChunkWHNF n
  = concat
  . (`using` parListWHNF)
  . map (`using` seqList)
  . chunk n
\end{verbatim}
where @chunk :: Int -> [a] -> [[a]]@ splits a list into chunks of
length @n@.  Having split the list into chunks, we then make a new
list in which each element forces the evaluation of the whole chunk,
before returning the chunk itself.  Then we @par@ each chunk using
@parListWHNF@, and finally concatenate the result.  This is almost
identical to the original strategy version, with the exception that it
may involve an extra traversal of the list for any parts that are not
evaluated in parallel, but the benefit is that there are no space
leaks.

A combinator that we find ourselves using often is @parBuffer@, which
behaves like @parList@ except that it doesn't traverse the whole list
eagerly; it sparks a fixed number of elements initially, and then
sparks subsequent elements as the list is consumed.  This formulation
works particularly well with programs that produce output as a lazy
list, since it allows us to retain the constant-space property of the
program while taking advantage of parallelism.  The disadvantage is
that we have to pick a buffer size, and the best choice of buffer size
might well depend on how many CPUs we have available.

Our modified version of @parBuffer@ that avoids space leaks is
@parBufferWHNF@:

\begin{verbatim}
parBufferWHNF :: Int -> [a] -> [a]
parBufferWHNF n xs = return xs (start n xs)
  where
    return (x:xs) (y:ys) = y `par` (x : return xs ys) 
    return xs [] = xs

    start !n [] = []
    start 0 ys = ys
    start !n (y:ys) = y `par` start (n-1) ys
\end{verbatim}

% ToDo: perhaps an alternative:
%     parweak :: a -> k -> b -> b
%   where 'a' is the spark, 'k' is the weak key.  While 'k' is alive,
%   the spark is alive.

\subsection{Compositional strategies revisited}

We can recover a more compositional approach to strategies
by changing their type.  The existing @Strategy@ type is defined thus:
% The combinators in the previous section suggest a modification to the
% strategies library with better space behaviour.  The key observation
% is that we need to retain the sparked structure so that we can ensure
% that each spark is either evaluated in a parallel thread, or
% \emph{fizzled} as as result of being evaluated by the main thread.
%The existing @Strategy@ type doesn't give us a way to do this, because
% it simply returns @done@ after applying the strategy:
\begin{verbatim}
  type Strategy a = a -> Done
\end{verbatim}
Suppose that instead we define @Strategy@ as a projection, like this:
\begin{verbatim}
  type Strategy a = a -> a
\end{verbatim}
then a @Strategy@ can do some evaluation and sparking, and return a
new @a@.  In order to use this new kind of @Strategy@ effectively, we
need a new version of the @par@ combinator:
\begin{verbatim}
spark :: Strategy a -> a -> (a -> b) -> b
spark strat a f = x `par` f x
  where x = strat a `pseq` a
\end{verbatim}
The @spark@ combinator takes a strategy @strat@, a value @a@, and a
continuation @f@.  It creates a spark to evaluate @strat a@, and then
passes a new object to the continuation with the same value as @a@.
When evaluated, this new object will cause the spark to fizzle and be
discarded.
% The implementation given here is not entirely satisfactory: if the
% main thread evaluates @a@ before the spark has been evaluated in
% parallel, the evaluation of @strat a@ will be forced.  Often this will
% just be extra work, but it means that @spark@ cannot be used for pure
% speculation.  However, it would be a perfectly reasonable semantics
% \emph{not} to evaluate @strat a@ in the main thread, but only in
% parallel threads.  Unfortunately we cannot define a version of @spark@
% with this behaviour, it has to be provided as a primitive.
Now we can recover compositional @parList@ and @seqList@ combinators:
\begin{verbatim}
parList :: Strategy a -> Strategy [a]
parList strat xs = foldr f [] xs
  where f x xs = spark strat x $ \x -> xs `pseq` x:xs

seqList :: Strategy a -> Strategy [a]
seqList strat xs = foldr seq ys ys
  where ys = map strat xs
\end{verbatim}
and indeed this works quite nicely.  Note that @parList@ creates
sparks in the same order as the list; to create sparks in reverse
order simply move the @xs `pseq`@ before the @spark@.  Also note that
@parList@ requires linear stack space, it is possible to write a
version that only requires linear heap space, but that requires two
traversals of the list.

Here is @parListChunk@ in the new style:
\begin{verbatim}
parListChunk :: Int -> Strategy a -> Strategy [a]
parListChunk n strat xs = ys `pseq` concat ys
  where ys = parList (seqList strat) $ chunk n xs
\end{verbatim}

% One final point: using this new formulation of strategies would let us
% use the WEAK policy for spark reclamation, eliminating the space leaks
% at the expense of a slightly more complicated framework for evaluation
% strategies.  With the WEAK policy in place, we are free to use the new
% formulation of strategies for speculative parallelism: it is safe to
% create sparks attached to parts of a structure that are not sure to be
% demanded, because we know that the spark can be garbage collected
% later.

% parMap is not tail-recursive.

% difference between parBuffer (lazyParList) and using chunks in mandelbrot.


\section{Making parallel programs run faster}

We now turn our attention from the programming model to the
implementation.  Our baseline is GHC 6.10.1, a mature Haskell
compiler.  Its performance on sequential code is very good, so the
overheads of parallelism are not concealed by sloppy sequential
execution.  It has supported parallel execution for several years, but
while parallel performance is sometimes good, it is sometimes
surprisingly bad.  The trouble is that it is hard to know \emph{why} it is
bad, because performance is determined by the interaction of four systems ---
the compiler itself, the GHC runtime system, the operating system, and the physical
hardware --- each of which is individually extremely complex.
This paper reports on our experience of improving both the absolute
performance and its consistency.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lrr}
\hline
\multicolumn{3}{c}{\textbf{Speedup on 4 cores}} \\
Program & Before & After \\
\hline
\input{speedup4}
\end{tabular}
&
\begin{tabular}{lrr}
\hline
\multicolumn{3}{c}{\textbf{Speedup on 7 cores}} \\
Program & Before & After \\
\hline
\input{speedup7}
\end{tabular}
\end{tabular}

\caption{Speedup results}
\label{f:speedup}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.47,viewport=50 200 800 600,clip]{speedup.pdf}
% bb = xl yl xh yh
% When diagram is in top LH corner of page,
%    x coords start at 0
%    y coords finish at max
\end{center}
\caption{Speedup results}
\label{f:speedup-graph}
\end{figure}

To whet your appetite, Figure~\ref{f:speedup} summarises the cumulative
improvement of the work we present, for 4 and 7 cores\footnote{Why did we use only 7 of the 8 cores on our test system?  In fact we
did perform the measurements for all 8 cores, but found that the
results were far less consistent than the 7 core results, and in some
cases performance degraded significantly.  On closer inspection the OS
appeared to be descheduling one or more of our threads, leading to
long pauses when the threads needed to synchronise.  This effect is
discussed in more detail in Section~\ref{s:viewer}.}.
Each table has 2 columns:

\begin{itemize}
\item {\bf Before:} speedup achieved by parallel execution using GHC 6.10.1,
compared to the same program compiled sequentially with 6.10.1, with the
  parallel GC turned off. In GHC 6.10.1 the parallel GC tended to make
  things worse rather than better,
  so this column reflects the best settings for GHC 6.10.1.
\item {\bf After:} our best speedup results, using the PARGC3 configuration
  (Section~\ref{s:parallel-gc}.
\end{itemize}
The improvements are substantial, especially for the most disappointing 
programs which actually ran slower when parallelism was enabled in 6.10.1.
Section~\ref{s:benchmarks} gives more details about the experimental 
setup and the benchmark programs.

Figure~\ref{f:speedup-graph} shows the scaling results for each
benchmark program after our cumulative improvements, relative to the
performance of the sequential version.

\section{Background: the GHC runtime}
\label{s:implementation}

By way of background, we describe in this section how GHC
runs Haskell programs in parallel.
In the sections that follow we shall present various measurements to
show the effectiveness of certain aspects of our implementation
design.  
Each of our measurements compare two configurations of GHC.  Many
of our improvements are cumulative and it proved difficult to untangle
the source-code dependencies from each other in order to be able to
make each measurement against a fixed baseline, so in each case we
will clearly state what the baseline is.

\subsection{The basic setup} \label{s:basic-setup}

The GHC runtime system supports millions of lightweight
threads by multiplexing them onto a handful of operating system
threads, roughly one for each physical CPU.  
This overall plan is well-established, but it is easier to sketch
than to implement!  

Each \emph{Haskell thread} runs on a finite-sized stack, which is
allocated in the heap. The state of a thread, together with its stack,
is kept in a heap-allocated \emph{thread state object} (TSO).  The
size of a TSO is around 15 words plus the stack, and constitutes the
whole state of a Haskell thread.  

A stack may grow by copying the TSO into a larger area, and may
subsequently shrink again.  In would be simpler to keep the TSO in a
separate object from the stack, because when resizing we currently
have to leave the old TSO in place with an indirection to the new TSO,
and we have to remember to check for these ``redirected'' TSOs.
However, when we experimented with separating the TSO and stack, we
found that the extra indirection caused a measurable reduction in
performance for intensive concurrency benchmarks.

\sdm{We should probably do stack-chunking anyway, for the GC benefits
  of not traversing the whole stack when it is deep.}\spj{Or give the 
write barrier a low-water mark?}

A Haskell thread is executed by an \emph{operating system thread}, or
\emph{worker thread}.  We maintain roughly one worker thread per
physical CPU, but exactly \emph{which} worker thread may vary from moment
to moment, as we explain in Section~\ref{s:foreign}.
Since the worker thread may change, we maintain exactly one
\emph{Haskell Execution Context} (HEC) for each CPU\footnote{
In the source code of the runtime system, a HEC is called a
``Capability''.  The HEC terminology comes from the lightweight
concurrency primitives work \cite{concsubstrate07}.}.
The HEC is a
data structure that contains all the data that an OS worker thread
requires in order to execute Haskell threads.  In particular, a HEC
contains
\begin{itemize}
\item An Ownership Field, protected by a lock, that 
records which worker thread is currently animating the capability
(zero if none is).  We explain in Section~\ref{s:foreign} 
why we do not use the simpler
device of a lock to protect the entire HEC data structure.

\item A Message Queue, containing requests from other HECs.  For
example, messages saying ``Please wake up thread T'' arrive here.

\item A Run Queue of threads ready to run.

\item An allocation area.

\item GC Remembered Sets.  Adding an object to the remembered set is a
  lock-free operation.

\item A Spark Pool.
Each invocation of $@par@~a~b$ adds the thunk $a$ to the (current HEC's) 
Spark Pool; this thunk is called a ``spark''.

\item A Worker Pool of spare worker threads, and a 
Foreign Outcall Pool of TSOs that are engaged in foreign calls (see Section~\ref{s:foreign}).

\end{itemize}
In addition there is a global Black Hole Pool, a set of
threads that are blocked on black holes (see Section~\ref{s:black-hole}).

An active HEC services work using the following priority scheme:

\begin{enumerate}
\item Service a message on the Message Queue.
\item Run a thread on the Run Queue; we use a simple round-robin scheduling order.
\item If any spark pool is non-empty, create a \emph{spark thread} and
  start running it (see Section~\ref{s:runningsparks}).
\item Poll the Black Hole Pool to see if any thread has become runnable;
if so, run it.
\end{enumerate}

All the state that a HEC needs for ordinary execution of Haskell
threads is local to the HEC, so under normal execution a HEC proceeds
without requiring any synchronisation, locks, or atomic instructions.
Synchronisation is only needed when:

\begin{itemize}
\item Load balancing is needed (Section~\ref{s:load-balancing}).
\item Garbage collection is required (Section~\ref{s:parallel-gc}).
\item Blocking on black holes (Section~\ref{s:blocking-on-blackholes}).
\item Performing an operation on an @MVar@, or an STM transaction.
\item Unblocking a thread on another HEC.
\item Throwing an exception to a thread on another HEC, or a blocked thread.
\item Allocating large or immovable memory objects; since these operations
are relatively rare, we allocate such objects from single global pool.
\item Making a (safe) foreign call (Section~\ref{s:foreign}).
\end{itemize}

% \sdm{Move this text somewhere: Each HEC maintains a private Run Queue,
%   Spark Pool, and so on, to avoid bottlenecking on a single shared
%   data structure.  However, having separate pools raises the usual
%  questions of how work gets spread out.}

\subsection{Memory management} \label{s:memory-mgt}

The memory manager is divided into two layers:

\begin{itemize}
\item The \emph{block allocator}, that allocates memory from the OS
  and supplies memory to the upper layer in the form of aligned
  \emph{blocks}.  Blocks are currently 4k, but the size is easily
  changed at compile-time.  Each block has an associated \emph{block
    descriptor}, which is a small table of data describing that block
  (for example, it contains a link field so that blocks can be linked
  together to form larger areas).  There is a cheap way (using
  arithmetic only) to map from a pointer to the block descriptor for
  its containing block.

\item The rest of the runtime manipulates memory in units of these
  blocks.  In particular, the garbage collector is generational and
  parallel; see \cite{parallel-gc-08} for details.
\end{itemize}

There is a single heap, shared among all the HECs, although each HEC
allocates into its own local allocation area.

\subsection{Foreign calls} \label{s:foreign}

Suppose that a Haskell thread makes a foreign call to a C procedure that blocks, such as
@getChar@.  We do not want the entire HEC to seize up so, before making the call,
the worker thread relinquishes ownership of the HEC, leaving the Haskell thread in 
a tidy state, and places it in the Foreign Outcall Pool so that the garbage collector
can find it.
We maintain a Worker Pool of worker threads for each HEC, each eager to become the worker that 
animates the HEC.  When one worker relinquishes ownership, it triggers a condition 
variable that wakes up another worker from the Worker Pool.  If the latter is empty,
a new worker is spawned.

What happens when the original worker thread W completes its call to
@getChar@ and wants to return?  To return, it must re-acquire
ownership of the HEC, so it must somehow displace any worker thread X
that currently owns the HEC.  To do so, it adds a message to the HEC's
Message Queue. When X sees this message, it signals W, and returns
itself to the worker pool.  Worker thread W wakes up, and takes
ownership of the HEC.  This approach is slightly better than directly
giving W ownership of the HEC, because W might be slow to respond, and
the HEC does not remain locked for the duration of the handover.

This also explains why we don't simply have a mutex protecting the
HEC, which all the spare worker threads are blocked on.  That approach
would afford us less control in the sense that we often want to hand
the HEC to a particular worker thread, and a simple mutex would not
allow us to do that.

Foreign calls are not the focus of this paper, but more details can be
found in \cite{concffi04}.

% ----------------------------------------------------
\section{Faster sparks}
\label{s:impl-spark-pool}

We now discuss the first set of improvements, which relate to
the handling of sparks.

\subsection{Sharing sparks} \label{s:work-stealing-sparks}

GHC 6.10.1 has a private Spark Pool for each HEC, but it uses a
``push'' model for sharing sparks, as follows.  On every iteration of
the main scheduler loop, each HEC checks whether its spark pool has
more than one spark.  If so, it checks whether any other HECs are idle
(a lock-free operation); if it finds an idle HEC it gives one or more
sparks to it, by temporarily acquiring ownership of the remote HEC and
inserting the sparks in its pool.

To make spark distribution cheaper and more asynchronous we
re-implementing each HEC's Spark Pool as a bounded work-stealing queue
\cite{Arora98threadscheduling,chaselev2005}.  A work-stealing queue is
a lock-free data structure with some attractive properties: the owner
of the queue can push and pop from one end without synchronisation,
meanwhile other threads can ``steal'' from the other end of the queue
incurring only a single atomic instruction.  When the queue is almost
empty, popping also incurs an atomic instruction to avoid a race
between the popping thread and a stealing thread.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{worksteal4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{worksteal7}
\hline
\end{tabular}
\end{tabular}

\caption{The effect of adding work-stealing queues vs. GHC 6.10.1}
\label{f:work-stealing}
\end{figure}

Figure~\ref{f:work-stealing} shows the effect of adding work-stealing
queues to our baseline GHC 6.10.1.  As we can see from the
results, work-stealing for sparks is almost always beneficial, and increasingly
so as we add more cores.  It is of particular benefit to \textbf{ray},
where the task granularity is very small.

\subsection{Choosing a spark to run} 
\label{s:choosing-sparks}

Because we use a work-stealing
queue for our spark pools, stealing threads must always take the
\emph{oldest} spark in the pool.  However, the HEC owning the spark
pool has a choice:
\begin{itemize}
\item It can take the youngest spark from the pool, which requires no
  atomic instructions, or
\item It can steal from its own pool, taking the oldest spark.  This
  requires an atomic instruction, however.
\end{itemize}

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{lifo4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{lifo7}
\hline
\end{tabular}
\end{tabular}

\caption{Using LIFO rather than FIFO for local sparks}
\label{f:lifo-fifo-sparks}
\end{figure}

Figure~\ref{f:lifo-fifo-sparks} shows the effect of changing the
default (FIFO) to LIFO.  In most of our benchmarks this results in
worse performance, because the older sparks tend to be ``larger''.

\sdm{I don't really understand these results, because in most of these
  programs there is only one thread creating sparks: parBuffer creates
  all the sparks in the main thread, so the main thread shouldn't be
  taking any sparks at all.  More investigation required.}

\subsection{Batching sparks}
\label{s:runningsparks}

To run a spark $a$, a HEC simply evaluates the thunk $a$ to head
normal form.  To do so, it needs a Thread State Object. It makes no
sense to create a fresh TSO for every spark, and discard it when the
evaluation is complete for the garbage collector to recover.

Instead, when there a HEC has no work to do, it checks whether there
are any sparks, either in the HEC's local spark pool or in any other
HEC's spark pool (check the non-empty status of a spark pool does not
require a lock).  If there are sparks available, then the HEC creates
a \emph{spark thread}, which is a perfectly ordinary thread except
that it runs the following steps in a loop:

\begin{enumerate}
\item If the local Run Queue or Message Queue is non-empty, exit.
\item Remove a spark from the local spark pool, or if that is empty,
  steal a spark from another HEC's pool.
\item If there were no sparks to steal, exit.
\item Evaluate the spark to weak-head-normal form.
\end{enumerate}

So a spark thread will run sparks one after another until it can find
no more sparks, or until there is other work to do, at which point it
exits.  This is a particularly simple strategy and works well: the
cost of creating the spark thread is amortized over multiple sparks,
and the spark thread gets out of the way quickly if any other work
arrives.  If a spark blocks on a black hole, since the spark thread is
just an ordinary thread it will block in the usual way, and the
scheduler will create another spark thread to continue running the
available sparks.  We don't have to worry unduly about having too many
spark threads, because a spark thread will always exit when there are
other threads around.  This reasoning does rely on sparks not being
too large, however: many large sparks becoming blocked could lead to a
large number of running spark threads.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{batch4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{batch7}
\hline
\end{tabular}
\end{tabular}

\caption{The effect of batching sparks}
\label{f:spark-batching}
\end{figure}


Figure~\ref{f:spark-batching} compares the effect of using the
spark-batching approach described above to the approach taken in GHC
6.10.1, which was to create a new thread for each activated spark.
Our baseline for these measurements is GHC 6.10.1 plus
work-stealing-queues (Section~\ref{s:work-stealing-sparks}).  Batching
sparks is particularly beneficial to two of our benchmarks,
\textbf{matmult} and \textbf{ray}, while it is a slight pessimisation
for \textbf{parfib} on 7 cores.  For \textbf{ray} the rationale is
clear: there are lots of tiny sparks, so reducing the overhead for
spark execution has a significant effect.  For \textbf{parfib} we
believe that the reduction in performance shown here is because the
program is actually being more effective at exploiting parallelism,
which leads to reduced performance due to lack of locality
(Section~\ref{s:parallel-gc}); as we shall see, this performance
loss is recovered by proper use of parallel GC.

% -----------------------------------------------------
\section{Garbage collection}
\label{s:parallel-gc}

When any HEC's allocation area is exhausted, a garbage collection must
be performed.  GHC 6.10.1 can perform parallel GC, but GC only takes place
when all HECs stop together, and agree to garbage collect.
We aim to keep this synchronisation 
overhead to a minimum by ensuring that we can stop a HEC
quickly (Section~\ref{s:context-switch}).  In future work we plan to
relax the stop-the-world requirement and adopt some form of
CPU-independent GC (Section~\ref{s:independent-gc}).

When a GC is required, we have the option of either
\begin{itemize}
\item Performing a single-threaded GC.  In this case, the HEC that
  initiated the GC waits for all the other HECs to cease execution,
  performs the GC, and then releases the other HECs.

\item Performing a parallel GC.  In this case, the initiating HEC
  sends a signal to the other HECs, which causes them to become GC
  threads and await the start of the GC.  Once they have all responded,
  the initiating HEC performs the GC initialisation and
  releases the other GC threads to perform GC.  When the GC
  termination condition is reached, each GC thread waits at the GC
  exit barrier.  The initiating HEC performs any post-GC tasks (such
  as starting finalizers), and then releases the GC threads from the
  barrier to continue running Haskell code.
\end{itemize}

In a single-threaded program, it is often better to use
single-threaded GC for the quick young-generation collections, because
the cost of starting up and shutting down the GC threads can outweigh
the benefits of doing GC in parallel.

\subsection{Avoiding synchronisation in parallel copying GC}

Parallel copying GC normally requires each GC thread to use an atomic
instruction to synchronise when copying an object, so that objects are
not accidentally duplicated.  The cost of these atomic instructions is
high: roughly 30\% of GC time \cite{parallel-gc-08}.  However, as we
suggested in that paper, it is possible to relax the synchronisation
requirement where immutable objects are concerned.  The only adverse
effect from making multiple copies of an immutable object is a little
wasted space, and we know from measurements that the rate of actual
collisions is very low--typically less than 100 collisions per second
of GC time--so the amount of wasted space is likely to be minuscule.

\begin{figure}
\begin{center}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{gclock}
\hline
\end{tabular}
\end{center}
\caption{Effect of locking all closures in the parallel GC}
\label{f:gclock}
\end{figure}

Our parallel GC therefore adopts this policy, and avoids synchronising
access to immutable objects.  Figure~\ref{f:gclock} compares the two
policies: the baseline is our current system in which we only lock
mutable objects, compared to a modified version in which we lock every
object during GC.  As the results show, our optimisation of only
locking mutable objects has a significant benefit on overall
performance: without it, performance drops by over 7\%.  The effect is
the most marked in benchmarks that do the most GC: \textbf{gray}, but
is negligible in those that do very little GC: \textbf{parfib}.

\subsection{Memory management of TSOs}

Since TSOs are mutable objects, even a minor garbage collection must
visit any TSO that has been written since the last time the collector
ran.  We therefore maintain a write barrier, so that only dirty TSOs---those
that have been executed since the last GC---are scanned.

In parallel GC, each HEC traverses its own local root set, which
consists of everything reachable from its own local data structures,
including the remembered sets.  Having local remembered sets turns out
to be particularly important here, since any thread which has run on
the current HEC will be in the local remembered set (because threads
are mutable), and will therefore be traversed by the HEC that ran it.
We don't necessarily traverse all threads on the run queue: a thread
that is in the old generation, and has not run (i.e. mutated) since
the last GC, need not be traversed.

A TSO may become unreachable, for example by blocking on an @MVar@
that is itself unreachable.  It would be correct to simply discard
such threads, since they can never become unblocked, but it makes
deadlocked programs very hard to debug.  Instead, we resurrect the
thread, and send it a @BlockedIndefinitely@ exception, a mechanism
akin to (and implemented by the same technique as) finalizers.

\subsection{Pre-emption and garbage collection}
\label{s:context-switch}

Since garbage collection is relatively frequent, and requires all HECs
to halt, it is important that they all do so promptly.  The standard
way to do this is to set the heap-limit register to zero, thus causing
the thread to return to the HEC scheduler when it next tries to
allocate.

On a register-poor machine, we keep the heap-limit ``register'' in
memory, in a block of ``registers'' pointed to by a single real
machine register.  In this case, it is easy for one HEC to set another
HEC's heap limit to zero, simply by zapping the appropriate memory
location.  On a register-rich machine we can keep the heap limit in a
real machine register, but it is then a good deal more difficult for
one HEC to zap another HEC's heap limit, since it is part of the
register set of a running operating-system thread.  We therefore
explored two alternatives for register-rich architectures:
\begin{itemize}
\item Keep the heap limit in memory. This slows the heap-exhaustion
check, but releases an extra register for argument passing.
\item Keep the heap limit in a register, and implement pre-emption by
  setting a separate memory-resident flag.  The flag is checked
  whenever the thread's current allocation block runs out, since it
  would be too expensive to insert another check at every heap-check
  point.  This approach is cheap and easy, but pre-emption is much
  less prompt: a thread can allocate up to 4k of data before noticing
  that a context-switch is required.
\end{itemize}

Figure~\ref{f:hplim} measures the benefit of using the heap-limit
register to signal a context-switch, versus checking a flag after each
4k of allocation.  We see a slight drop in performance at 4 cores,
changing to an increase in performance at 7 cores.  This technique
clearly becomes more important as the number of cores and the amount
of garbage collection increases: benchmarks like \textbf{gray} that do
a lot of GC benefit the most.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{hplim4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{hplim7}
\hline
\end{tabular}
\end{tabular}
\caption{Using the heap-limit for context switching}
\label{f:hplim}
\end{figure}

\subsection{Parallel GC and locality} \label{s:locality}

When we initially developed the parallel GC, our goal was to improve
GC performance, so we focused most of our effort on using parallelism
to accelerate garbage collection for \emph{single-threaded} programs
\cite{parallel-gc-08}.  In this case the key goal is achieving good
load-balancing, that is, making sure that all of the GC threads have
work to do.  

However, there is another factor working in the
opposite direction: locality.  For parallel programs, when GC begins
each CPU already has a lot of data in its cache; in a sequential 
program only one CPU does.  It would obviously make sense for
each CPU to garbage-collect its own data, so far as possible, rather
than to allow GC to redistribute it.

Each HEC starts by tracing its own root set, starting from the 
HEC's private data (Section~\ref{s:basic-setup}).  However, our
original parallel GC design used global work queues for
load-balancing \cite{parallel-gc-08}.  This is a poor choice for locality, because the link
between the CPU that copies the data and the CPU that scans it for
roots is lost.  To tackle this, we modified our parallel GC design to
use work-stealing queues.  The benefits of this are threefold:

\begin{enumerate}
\item Contention is reduced.
\item Locality is improved: a CPU will take work from its own queue in
  preference to stealing.  Local work is likely to be in the CPU's
  cache, because it consists of objects that this CPU recently copied.
\item We can easily disable load-balancing entirely, by opting not to
  steal any work from other CPUs.  This trades parallelism in the GC
  for locality.
\end{enumerate}

\begin{figure}
\begin{center}
\begin{tabular}{lrrrr}
\hline
        & \multicolumn{3}{c}{$\Delta$ Time (\%)} \\
Program & PARGC1 & PARGC2 & PARGC3 & \\
\hline
\input{pargc4}
\hline
\end{tabular}
\caption{The effectiveness of parallel GC (4 cores)}
\label{f:parallel-gc4}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{lrrrr}
\hline
        & \multicolumn{3}{c}{$\Delta$ Time (\%)} \\
Program & PARGC1 & PARGC2 & PARGC3 & \\
\hline
\input{pargc7}
\hline
\end{tabular}
\end{center}
\caption{The effectiveness of parallel GC (7 cores)}
\label{f:parallel-gc7}
\end{figure}

The use of work-stealing queues for load-balancing in parallel GC is a
well-known technique \cite{flood:parallel}, however what has not been
studied before is the trade-off between whether to do load-balancing at
all or not for parallel programs.  

We will measure our benchmarks in three configurations:

\begin{itemize}
\item The baseline is our best system, with parallel GC turned off.
\item PARGC1: using parallel GC in the old generation only, with
  load-balancing.
\item PARGC2: using parallel GC in both young and old generations, with load-balancing.
\item PARGC3: using parallel GC in both young and old generations,
  without load-balancing.
\end{itemize}

PARGC1 and PARGC2 use work-stealing for load-balancing, PARGC3 uses no
load-balancing.  In terms of locality, PARGC2 will improve locality
significantly by traversing most of the data reachable by parallel
threads on the same CPU as the thread is executing.  PARGC3 will
improve locality further by not moving data from one CPU's cache to
another in an attempt to balance the work of GC.

Figures~\ref{f:parallel-gc4} and \ref{f:parallel-gc7} present the
results, for 4 cores and 7 cores respectively. There are several
aspects to these figures that are striking:

\begin{itemize}
\item \textbf{partree} delivers an 80\% improvement with PARGC3 on 7
  cores, with most of the benefit coming with PARGC2.  Clearly
  locality is vitally important in this benchmark.
\item \textbf{gray} and \textbf{mandel} degrade significantly with
  PARGC2, recovering with PARGC3.  Load-balancing appears to be having
  a significant negative effect on performance here.  These are
  benchmarks that don't achieve full speedup, so it is likely that
  when a GC happens, idle CPUs are stealing data from the busy CPUs,
  harming locality more than would be the case if all the CPUs were
  busy.
\item PARGC3 is almost always better than the other configurations.
\end{itemize}

Future versions of GHC will use PARGC3 by default for parallel
execution, at least until we experiment with \emph{independent GC}
(Section~\ref{s:independent-gc}).

% -----------------------------------------------------
\section{Thunks and black holes}
\label{s:black-hole}

\subsection{Mutual exclusion}
\label{s:eager-blackholing}

Suppose that two Haskell threads, A and B, begin evaluation of a thunk
$t$ simultaneously.  Semantically, it is acceptable for them both to
evaluate $t$, since they will get the same answer \cite{multiproc05};
but operationally it is better to avoid this duplicated work.  The
obvious way to do so is to lock every thunk when starting evaluation,
but that is expensive; measurements in the earlier cited work
demonstrate an increase in execution time of around 50\%.  So we
considered several variants that trade a reduced overhead against the
risk of duplicated work:
\begin{description}
\item[EagerBH:] immediately on entry, thread A overwrites $t$ with a \emph{black hole}.
If thread B sees a black hole, it blocks until A performs the update
(Section~\ref{s:blocking-on-blackholes}). 
The ``window of vulnerability'', in which a second thread might start
a duplicate evaluation, is now just a few instructions
wide.  The cost compared to sequential execution is an extra
memory store on every thunk entry.

\item[RtsBH:] enlists the runtime system, using
the scheme described in \cite{multiproc05}. The idea is to
walk a thread's stack whenever it returns to the scheduler, and
``claim'' each of the thunks under evaluation using an atomic
instruction.  If a thread is found to be evaluating a thunk already
claimed by another thread, then we suspend the current execution and
put the thread to sleep until the evaluation is complete.  Since every 
thread will return to the scheduler at regular intervals (say, to do
garbage collection), this ensures that we cannot continue to evaluate
the same thunk in multiple threads indefinitely.  But the overhead
is much less than locking every thunk because most thunks are entered,
evaluated, and updated during a single scheduler time-slice.

\item[PostCheck:] 
As \cite{multiproc05} points out, if two threads both succeed in \emph{completing}
the evaluation of the same thunk, and its value itself contains more thunks,
there is a danger that an unbounded amount of work can be duplicated.
The \textbf{PostCheck} strategy adds a test just before the 
update to check whether the thunk has already been updated by another thread.
This test does not use an atomic instruction, but reduces the chance
of further duplicate work taking place.
\end{description}
In our earlier work \cite{multiproc05} we measured of the overheads of
locking every thunk, but said nothing about the overheads or
work-duplication of the other strategies.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{eager4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{eager7}
\hline
\end{tabular}
\end{tabular}
\caption{The effect of eager black-holing}
\label{f:eager}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{lrr}
\hline
        &       & RtsBH + \\
Program & RtsBH & EagerBH \\
\hline
\input{suspensions}
\hline
\end{tabular}
\end{center}
\caption{The number of duplicate computations caught}
\label{f:suspensions}
\end{figure}

GHC 6.10.1 implements \textbf{RtsBH}.  Figure~\ref{f:eager} shows the
additional effect of using eager black-holing on our benchmark
programs, for 4 and 7 cores.  As you might expect, the effect is
minor, because \textbf{RtsBH} catches almost all the cases that
\textbf{EagerBH} does, except for very short-lived thunks which do not
matter much anyhow.  Figure~\ref{f:suspensions} shows how many times
\textbf{RtsBH} catches a duplicate computation in progress, both with
and without adding \textbf{EagerBH}.  As we can see, without
\textbf{EagerBH} there are occasionally a substantial number of
duplicate evaluations (eg. in \textbf{ray}), but \textbf{EagerBH}
reduces that number to almost zero.  In \textbf{ray}, although we managed
to eliminate a large number of duplicate evaluations using
\textbf{EagerBH}, the effect on overall execution time was negligible:
this program creates $10^6$ tiny sparks, so 1200 duplicate evaluations
has little impact.  In fact, with the fine granularity in this
benchmark, it may be that the cost of suspending the duplicate
evaluation and blocking the thread outweighs the cost of just
duplicating the computation.

To date we have not measured the effect of \textbf{PostCheck}.  We
expect it to have no effect on these benchmarks, especially in
combination with \textbf{EagerBH}.  However, we have experienced the
effect of unbounded duplicate work in other programs; one good example
where it can occur is in this version of @parMap@:

\begin{verbatim}
parMap :: (a -> b) -> [a] -> [b]
parMap f [] = []
parMap f (x:xs) = fx `par` (pmxs `par` (fx:pmxs))
   where fx = f x
         pmxs = parMap f xs
\end{verbatim}
This function sparks both the head and the tail of the list, instead
of traversing the whole list sparking each element as in the usual
@parMap@.  The advantage of this version is that the list remains
lazy, and it scales smoothly to create as much parallelism as there
are CPUs available; it has similar properties to @parBuffer@ in this
respect.  The duplication problem occurs if two threads evaluate the
@pmxs@ thunk: then the tail of the list is duplicated, possibly
resulting in a large number of useless sparks being created.  It is
interesting to note that the useless sparks would be eliminated by the
garbage collector under the WEAK policy, however
(Section~\ref{s:space-par}).

\subsection{Blocking on a black hole}
\label{s:blocking-on-blackholes}

When a thread A tries to evaluate a black hole, it must block until
the thread currently evaluating the black hole (thread B) completes the
evaluation, and overwrites the thunk with (an indirection to) its
value.  In earlier implementations (before 6.10) we arranged that thread B would
attach its TSO to the thunk, so that thread A could re-awaken B when
it performed the update.  But that requires expensive synchronisation
on every update, in case the thunk by now has a sleeping thread
attached to it.

Since thunk updates are very common, but collisions (in which a
sleeping thread attaches itself to a thunk) are very rare, GHC 6.10 instead
optimises for the common case.  Instead of attaching itself to the
thunk, the blocked thread B simply polls the thunk, waiting for the
update.  Since a thunk can only be updated once, an update can
therefore be performed without any synchronisation whatsoever,
provided that writes are not re-ordered. We can even get away without
a synchronisation instruction when starting thunk evaluation, if we
accept a tiny chance that the same thunk will be evaluated more than
once.  Our earlier work \cite{multiproc05} discusses these
synchronisation issues in much more detail.

There are a variety of ways to implement the black-hole polling mechanism:
\begin{itemize}
\item GHC 6.10 maintains a single, global Black Hole Pool, which 
the HECs poll they are otherwise idle, and at least once per GC.
\item Alternatively, each HEC could maintain a private Black Pool.
\item Another possibility is for the blocked thread to sleep for a while
  before trying the thunk again. This would make it easy to experiment
  with back-off strategies.
\end{itemize}

\spj{Measurements of the size of the global BH pool.}
\spj{Measurements of our claim that collisions are rare.  Are these in the other
paper.}


% ------------------------------------------------
\section{Load balancing and migration} \label{s:load-balancing}

In this section we discuss design choices concerning which HEC should
run which Haskell threads.

\subsection{Sharing runnable threads}

In the current implementation, while we (now) use work-stealing for \emph{sparks},
we use work-pushing for \emph{threads}.  That is, when a HEC detects that it
has more than one thread in its Run Queue and there are other idle
HECs, it distributes some of the local threads to the other HECs.  

The reason for this design is mostly historical: we could represent
the Run Queue using a work-stealing queue and thereby use work-stealing
for load-balancing of threads.  However, threads tend to be less
fine-grained than sparks, and so a more heavyweight mechanism is to
some extent justified.  Another justification is that the Run Queue is
``just another list of TSOs'', of which there are many: lists of
blocked threads can be attached to @MVar@s, @TVars@, and other TSOs
(when throwing an exception to another thread).  TSOs have a link
field precisely for linking them together into lists, and furthermore
the link field is subject to the write barrier, so the GC may not have
to traverse parts of the Run Queue that have not changed recently.  To
use a work-stealing queue for the Run Queue would certainly add some
complexity, even using the existing work-stealing queue abstraction.

\begin{figure}
\hspace{-2ex}
\begin{tabular}{ll}
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{4 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{nomigrate4}
\hline
\end{tabular}
&
\begin{tabular}{lr}
\hline
\multicolumn{2}{c}{\textbf{7 cores}} \\
Program & $\Delta$ Time (\%) \\
\hline
\input{nomigrate7}
\hline
\end{tabular}
\end{tabular}
\caption{Disabling thread migration}
\label{f:migrate}
\end{figure}

Figure~\ref{f:migrate} shows the effect of disabling automatic thread
migration, against a baseline of the PARGC3 configuration
(Section~\ref{s:locality}).  Since these are parallel, rather than
concurrent, programs, the only way that multiple threads can exist on
the Run Queue of a single CPU is when a thread becomes temporarily
blocked (on a blackhole, Section~\ref{s:blocking-on-blackholes}), and
then later becomes runnable again.  As we can see from the results,
often allowing migration makes no difference.  Occasionally it is
essential: for example \textbf{matmult} on 4 cores.  And occasionally,
as in \textbf{ray}, allowing migration leads to worse performance,
probably due to lost locality.

Whether to allow migration or not is a runtime flag, so the programmer
can experiment with both settings to find the best one.

\subsection{Migrating on wakeup}

A blocked thread can be woken up for various reasons: if it is blocked
on a black hole, it is woken up when some HEC notices that the black
hole has now been evaluated (Section~\ref{s:blocking-on-blackholes});
if it is blocked on an empty @MVar@, then it can be unblocked when
another thread performs a @putMVar@ operation on that @MVar@.

When a thread is woken up, if it was previously running on another
HEC, we have a choice: it can be placed on the Run Queue of the
current HEC (hence migrating it), or we could arrange to awaken it on
the HEC it was previously running on.  In fact, we could wake it up on
any HEC, but typically these two options are the most profitable.

Moving the thread to the current HEC might be advantageous if the
thread is involved in a series of communications with another thread
on this HEC: context-switching between two threads on the same HEC is
particularly cheap.  However, locality might also be important: the
thread might be referencing data that is in the cache of the other HEC
(a HEC should be thought of as a CPU in this regard).

In GHC we take locality seriously, so our default is to not migrate
awoken threads to the current CPU.  For parallel programs it is never
worthwhile to change this setting: since threads only block on
blackholes, and threads blocked on blackholes get woken up by
whichever CPU is checking the blackhole pool, we always want to awaken
the thread on the last CPU it was running on.

% -----------------------------------------------------
\section{Benchmarks and experimental setup} \label{s:benchmarks}

Our test system consists of 2 quad-core Intel Xeon(R) E5320 processors
at 1.6GHz.  Each pair of cores shares 4MB of L2 cache, and there is
16GB of system memory.  The system was running Fedora 9.  Although the
OS was running in 64-bit mode, we used 32-bit binaries for our
measurements (programs compiled for 64-bit tend to place more stress
on the memory system and garbage collector resulting in less
parallelism).  In all cases we ran the programs five times and took
the average wall-clock execution time.

Our benchmarks consist of a selection of small-to-medium-sized
Parallel Haskell programs from various sources:

\begin{itemize}
\item Old benchmarks for the GUM system (parfib
\item More recent benchmarks used to compare GHC with Eden
  \cite{berthold:comparing} (sumeuler, matmult)
\item New benchmarks (for example, we did a simple parallelisation of
  an existing mandelbrot set program).
\end{itemize}

In particular, the benchmark programs we use are:

\begin{itemize}
\item \textbf{parfib}: the ubiquitous parallel fibonacci function,
  included here as a sanity test to ensure that our implementation is
  able to parallelise micro-benchmarks.  The parallelism is
  divide-and-conquer-style, using explicit @par@ and @pseq@.
\item \textbf{sumeuler}: the sum of the value of Euler's function
  applied to each integer up to a given bound.  This is a map/reduce
  style problem: applications of the Euler function can be performed
  in parallel, and the results must be summed
  \cite{trinder:parallel}.  The parallelism is expressed using
  @parListChunk@ from the strategies library.
\item \textbf{matmult}: A naive matrix-multiply algorithm.  The matrix
  is represented as a @[[Int]]@.  The parallelism is expressed using
  @parListChunk@.
\item \textbf{ray}: A ray-tracer benchmark\footnote{This program has a
  long history. According to comments in the source code, it was
  ``taken from Paul Kelly's book, adapted by Greg Michaelson for SML,
  converted to (parallel) Haskell by Kevin Hammond''.}.  The
  parallelism is expressed using @parBuffer@, and is quite
  fine-grained (each pixel to be rendered is a separate spark).
\item \textbf{gray}: Another ray-tracing benchmark, this time taken
  from an entry\footnote{from the Galois team} in the ICFP'00
  programming contest.  Only the rendering part of the program has
  been parallelised, using a @parBuffer@ as above.  According to time
  profiling, the program only spends about 50\% of its time in the
  renderer, so we expect this to limit the parallelism we can achieve.
  The parallelism is expressed using a single @parBuffer@ in the
  renderer.
\item \textbf{prsa}: A parallel RSA message encoder, encoding a 500KB
  message.  Parallelism is again expressed using @parBuffer@.
\item \textbf{partree}: A parallel map and fold over a tree.  The
  program originates in the GUM benchmark suite, and in fact appears
  to be badly written: it is quadratic in the size of the tree.
  Nevertheless, it does appear to run in parallel, so we used the
  program unmodified for the purposes of benchmarking.
\item \textbf{mandel}: this is a mandelbrot-set program originating in
  the @nofib@ benchmark suite \cite{partain:nofib}.  It generates a lazy list
  of pixel data (for a 1024x1024 scene), in a similar way to the ray
  tracer, and it was parallelised in the same way with the addition of
  @parBuffer@.  The difference in this case is that the parallelism is
  more coarse-grained: each scan-line of the result is a separate
  spark.
\end{itemize}

While some of these benchmarks (e.g. parfib, sumeuler) are competitive
in absolute performance with sequential programs that you might write
in either Haskell or another language, most of these programs will lag
significantly compared to a heavily-optimised sequential version.  The
reason is that many programs here are written in idiomatic Haskell
code, usually with no attempt to optimise for sequential performance.
Our goal in this work has not been to optimise the \emph{programs},
but rather to optimise the execution engine to make existing programs
parallelise better.  Having said that, it may well be that if these
programs were optimised for sequential performance that the
improvements we have made in parallel execution performance may not
carry through, and other bottlenecks may surface.

Some of these programs were already optimised to run well with GUM
(sumeuler, matmult, parfib).  In the other programs we have made a
small amount of effort to modify the program so that it runs in
parallel (ray, gray, mandel, prsa).  Our efforts included trying a few
different strategies, but by no means have we attempted to thoroughly
optimise these programs for parallelism: the focus here is on
improving parallelism by modifying the runtime and execution engine,
so starting with programs that have already been tuned to work around
the bottlenecks in the runtime would be self-defeating.

The programs are all small, and most are trivially parallelisable.
The lack of larger benchmarks is a shortcoming that we hope to
remedy in future work.  Nevertheless smaller benchmarks have
their uses:

\begin{itemize}
\item Small benchmarks show up in high relief
  interesting differences in the behaviour of our runtime and
  execution model.  Differences which would be less marked had we
  used only large programs.
\item We know that most of these programs \emph{should} parallelise
  well, so any lack of parallelism is more likely to be as a result of
  choices made in the language implementation than in the program
  itself.  
\end{itemize}

\subsection{Profiling}
\label{s:viewer}

To help understand the behaviour of our benchmark programs we
developed a graphical viewer for event information generated by the
runtime system. The viewer is modeled after circuit waveform viewers
with a profile drawn with time on the $x$-axis and HEC number on the
$y$-axis.  In each HEC's timeline, the bar is coloured green when the
HEC is running Haskell code, and orange when it is garbage collecting.
Events, such as threads blocking or being woken up, are indicated by
labels annotating the timeline when the view is zoomed enough.

This visualisation of the execution is immensely useful for being able
to quickly identify problem areas.  For example, when we ran our
benchmarks on all 8 cores of our 8-core machine, we experienced
inexplicable drops in performance.  Figure~\ref{f:problem} shows the
problem area as seen in the profile viewer.  In the middle of the
picture there is a long period where one HEC has initiated a GC, and
is waiting for the other HECs to stop.  The initiating/waiting HEC has
a white bar, the HECs that have already stopped and are ready to GC
are coloured orange.  One HEC is green during this period, indicating
that it is apparently running Haskell code and has not responded to
the call for a GC.  In fact, the OS thread running this HEC has been
descheduled by the OS, so does not respond for a relatively long
period.  The same pattern repeats many times during the execution,
having a significant impact on the overall runtime.

\begin{figure}
\begin{center}
\includegraphics[width=8.5cm]{problem.pdf}
\end{center}
\caption{A slow synchronisation}
\label{f:problem}
\end{figure}

This experience does illustrate that our runtime is particularly
sensitive to problems such as this due to the relatively high
frequency of full synchronisations needed for GC, and that tackling
independent GC (Section~\ref{s:independent-gc}) should be a high
priority.

% ----------------------------------------
% \section{Configurations}
% 
% The aim is to measure the difference between various configurations of
% our runtime system and execution model.  The configurations we measure
% are as follows.  The modifications are cumulative; that is, each
% configuration is constructed from the previous configuration with the
% noted modifications.
% 
% \begin{itemize}
% \item BASE: the baseline system, essentially GHC 6.10.1.
% \item WS: after introducing work-stealing for load-balancing of sparks
%   (Section~\ref{s:impl-spark-pool}).
% \item BATCH: running sparks one after another in a single thread,
%   without creating a new thread for each one
%   (Section~\ref{s:runningsparks})
% \item HPLIM: using the heap-limit to trigger context switching
%   (Section~\ref{s:context-switch}).
% \item EBH: adding eager blackholing
%   (Section~\ref{s:eager-blackholing}).
% \item PARGC1: adding parallel GC in the old generation
% \item PARGC2: adding parallel GC in the young generation
% \item PARGC3: turning off load-balancing in the parallel GC
% \item NM: no migration: turning off automatic thread migration for load-balancing
% \item WM: adding migration on thread wakeup
% \item LIFO: local sparks are taken in LIFO order, rather than FIFO
% \end{itemize}
% 
% 

% \section{Benchmarks}
% \label{s:benchmarks}
% 
% Our benchmarks are drawn from several sources:
% 
% \begin{itemize}
% \item Old benchmarks for the GUM system (parfib
% \item More recent benchmarks used to compare GHC with Eden
%   \cite{berthold:comparing} (sumeuler, matmult)
% \item New benchmarks (for example, we did a simple parallelisation of
%   an existing mandelbrot set program).
% \end{itemize}
% 
% The sections below describe the pertinent details of each of our
% benchmark programs.  While some of these benchmarks (e.g. parfib,
% sumeuler) are competitive in absolute performance with sequential
% programs that you might write in either Haskell or another language,
% most of these programs will lag significantly compared to a
% heavily-optimised sequential version.  The reason is that many
% programs here are written in idiomatic Haskell code, usually with no
% attempt to optimise for sequential performance.  Our goal in this work
% has not been to optimise the \emph{programs}, but rather to optimise
% the execution engine to make existing programs parallelise better.
% Having said that, it may well be that if these programs were optimsed
% for sequential performance that the improvements we have made in
% parallel execution performance may not carry through, and other
% bottlenecks may surface.
% 
% The value of our measurements is not to show that we can parallelise,
% say, the mandelbrot set program, but rather to show that we can
% realise parallelism in a wider range of Haskell programs than before,
% and to identify the key areas that we need to focus on in the future.
% 
% \subsubsection{parfib}
% The ubiquitous parallel fibonacci function, included here as a sanity
% test to ensure that our implementation is able to parallelise
% microbenchmarks, and to test the effect of modifying granularity.
% Even this small benchmark shows some interesting variation with some
% runtime configurations.
% 
% The core of the parfib benchmark is this code:
% 
% {\small
% \begin{verbatim}
% parfib :: Int -> Int -> Int
% parfib n t | n <= t = nfib n
%            | otherwise = n1 `par` (n2 `pseq` n1 + n2 + 1)
%            where n1 = parfib (n-1) t
%                  n2 = parfib (n-2) t
% \end{verbatim}
% }
% 
% where @nfib@ is the purely sequential version of the function.  The
% argument @t@ is a threshold, which controls how deep we descend in the
% tree creating parallel tasks, before switching to sequential
% execution.  With a high threshold, we create a few large tasks, with a
% low threshold we create many small tasks.
% 
% The default parameters we use for this benchmark are: @n=43@ and @t=11@.
% 
% \subsubsection{sumeuler}  
% 
% The sum of the value of Euler's function applied to each integer up to
% a given bound.  This is a map/reduce style problem: applications of
% the Euler function can be performed in parallel, and the results must
% be summed \cite{trinder:parallel}.
% 
% The core of the algorithm is:
% 
% {\small
% \begin{verbatim}
% sumEuler  :: Int -> Int -> Int
% sumEuler c n = sum ([ (sum . map euler) x 
%                     | x <- splitAtN c [n,n-1..0]]
%                     `using` parList rnf)
% \end{verbatim}
% }
% the list of @n@ integers is split into chunks of size @c@.  For each
% chunk, we map the @euler@ function over the list and sum the results;
% the main thread calculates the sum of the result of summing each
% chunk.  The chunks are processed in parallel using @parList@.
% 
% The default parameters we use are @c=100@ and @n=8000@, creating 80
% tasks.
% 
% \sdm{Meausre the effect of using a better @parList@ here?}
% 
% \subsubsection{matmult} 
% 
% A naive matrix-multiply algorithm.  The matrix is represented as a
% @[[Int]]@.  Two initial matrices are generated, the second one is
% transposed, and then the algorithm proceeds with @multMatricesTr@:
% 
% \begin{verbatim}
% multMatricesTr :: Matrix -> Matrix -> Matrix
% multMatricesTr m1 m2 = 
%    [ [addProd row col 0 | col <- m2] 
%    | row <- m1]
% 
% addProd :: Vector -> Vector -> Int -> Int
% addProd (v:vs) (w:ws) !acc 
%   = addProd vs ws (acc + v*w)
% addProd _ _ !n = n
% \end{verbatim}
% 
% The parallelism is expressed as a @parListChunk@ of the result matrix;
% that is, each task is a chunk of lines of the result matrix.  The
% default settings are: 600x600 matrices, with a chunk size of 10.
% 
% \subsubsection{ray} 
% 
% A ray-tracer benchmark with a long history.  According to comments in
% the source code, it was ``taken from Paul Kelly's book, adapted by Greg
% Michaelson for SML, converted to (parallel) Haskell by Kevin Hammond''.
% 
% Ray tracing obviously parallelises well, since the colour of each
% pixel can be computed independently of all the others.  The challenge,
% however, is to do it in constant space.  The key part of the algorithm
% that we want to parallelise is this:
% 
% \begin{verbatim}
% findImpacts :: [Ray] -> [Object] -> [Impact]
% findImpacts rays objects = map (firstImpact objects) rays
% \end{verbatim}
% where @rays@ is a list of pixel coordinates, and @objects@ represents
% the scene to render.  The avlue of each pixel is calculated by the
% function @firstImpact@, which calculates which of the objects in the
% scene is first impacted by a light ray eminating from the given
% coordinates on the visual plane.
% 
% Thanks to Haskell's laziness, the algorithm runs in constant space:
% the input list @rays@ is a lazy list, and @findImpacts@ generates a
% lazy list of results.  Each pixel is calculated on demand.  However,
% if we try to parallelise using the obvious @parList@, we lose this
% property: @parList@ is tail-strict, and so will evaluate the entirety
% of @rays@ before any results can be generated.
% 
% Instead we need a different strategy.  The need for this was
% recognised by Trinder et. al. \cite{trinder:strategies} who suggested
% @parBuffer@:
% 
% \begin{verbatim}
% parBuffer :: Int -> Strategy a -> [a] -> [a]
% parBuffer n s xs = return xs (start n xs)
%   where
%     return (x:xs) (y:ys) = (x : return xs ys) 
%                            `sparking` s y
%     return xs [] = xs
% 
%     start n [] = []
%     start 0 ys = ys
%     start n (y:ys) = start (n-1) ys `sparking` s y
% \end{verbatim}
% 
% And indeed this works quite nicely, although the addition of
% @parBuffer@ does hurt sequential performance by about 15\%.  The
% reason is that none of the sparks are garbage collected (see
% Section~\ref{s:space-par}).  So in the version we test here, we modify
% @parBuffer@ to avoid using strategies:
% 
% \begin{verbatim}
% parBuffer :: Int -> [a] -> [a]
% parBuffer n xs = return xs (start n xs)
%   where
%     return (x:xs) (y:ys) = y `par` (x : return xs ys) 
%     return xs [] = xs
% 
%     start !n [] = []
%     start 0 ys = ys
%     start !n (y:ys) = y `par` start (n-1) ys
% \end{verbatim}
% 
% This version has virtually no impact on the sequential performance, as
% each spark ``fizzles'' as the main thread evaluates the corresponding
% list element.  \sdm{measurements?}
% 
% \sdm{measurements of the buffer size?}
% 
% \subsubsection{gray}
% 
% Another ray-tracing benchmark, this time taken from an entry in the
% ICFP'00 programming contest.  Only the rendering part of the program
% has been parallelised, using a @parBuffer@ as above.  According to
% profiling, the program spends about half its time in the renderer, so
% we expect this to limit the parallelism we can achieve.
% 
% \subsubsection{prsa} 
% 
% A parallel RSA message encoder, encoding a 500KB message.  This
% program also has its origins in the GUM benchmark suite, and uses a
% pre-strategies version of @parmap@:
% 
% \begin{verbatim}
% parmap :: (a -> b) -> [a] -> [b]
% parmap f [] = []
% parmap f (x:xs) = fx `par` (pmxs `par` (fx:pmxs))
%    where fx = f x
%          pmxs = parmap f xs
% \end{verbatim}
% 
% This @parmap@ has the interesting property that it is not tail-strict
% in the list; it creates one spark to evaluate the head of the list,
% and another spark to evaluate @parmap@ on the tail of the list.  So in
% the sequential case this will not impact space behaviour, but when
% there are parallel proessors available, the rest of the list may be
% explored in parallel.  In fact this @parmap@ works rather well, and
% performs comparably to @parBuffer@.  It has the additional advantage
% of being more adaptive, in that there is no need to specify a buffer
% size.  However, @parmap@ is more dependent on the granularity of the
% tasks: with the fine-grained tasks in \textbf{ray}, it performs
% miserably. \sdm{The MUT time goes up; perhaps duplication of work -
%   it's possible that if one of the @parmap@ thunks gets duplicated
%   then we can duplicate the whole list!}
% 
% \subsubsection{mandel} 
% 
% This program originates in the @nofib@ benchmark suite \cite{nofib}.
% It generates a lazy list of pixel data, in a similar way to the ray
% tracer, and it was parallelised in the same way with the addition of
% @parBuffer@.

% \section{Implementation notes}
% 
% \sdm{Older notes: things we might want to talk about.}
% 
% \subsection{Lightweight concurrency (forkIO)}
% 
% \begin{itemize}
% \item We also have lightweight communication models: atomicModifyIORef
%   and MVar, and STM for higher-level but heavyweight communication.
% \end{itemize}
% 
% \subsection{Scheduling policies, thread migration}
% 
% \begin{itemize}
% \item We currently do round-robin scheduling and try hard to avoid
%   starvation: newly woken threads go to the back of the queue.
% \item No priority scheduling: keep things simple.
% \item No notion of thread hierarchies or parents/children.
% \end{itemize}
% 
% \subsection{Foreign Function Interface support}
% 
% \begin{itemize}
% \item We need a pool of worker threads for each Capability.  A Haskell
%   thread releases the Capability to another worker when it makes a
%   foreign call.
% \item Foreign calls may call back to Haskell: a Task represents each
%   in-call.
% \item Bound threads: a TSO may be bound, which means an explicit
%   thread switch in order to run it.
% \end{itemize}
% 
% \subsection{Asynchronous exceptions (throwTo)}
% 
% \begin{itemize}
% \item throwTo is useful for explicit speculative parallelism
%   (c.f. Conal's amb/unamb operators, which race threads against each
%   other).
% \item throwTo is synchronous: doesn't return until the exception has
%   been delivered.
% \item If two threads throwTo each other, only one succeeds.
% \item Difficult to get right in a multithreaded environment.
% \end{itemize}
% 
% 
\input{related-work}

\section{Conclusion}
\label{s:conclusion}

\section{Future work}

Re-do FDIP, we might get better results now.

Replacement for strategies and/or par.

\subsection{Independent GC}
\label{s:independent-gc}

Stop-the-world GC will inevitably become a bottleneck as the number of
cores increases.  There are known techniques for doing CPU-independent
GC \cite{doliguez:concurrent}, and these techniques are used in
systems such as Manticore \cite{fluet:manticore}.  

We fully intend to pursue CPU-independent GC in the future.  However
this is unlikely to be an easy transition.
CPU-independent GC replaces direct sharing by physical separation and
explicit communication.  This clearly leads to trade-offs; it isn't a
straightforward win.  More specifically, CPU-independent GC requires a
local-heap invariant, namely that there are no pointers between local
heaps, or from the global heap into any local heap.  Ensuring and
maintaining this invariant introduces new costs and complexities into
the runtime execution model.  

On the other hand, as the number of cores in modern CPUs increases,
the illusion of shared memory begins to break down.  We are already
experiencing severe penalties for losing locality (Section~\ref{s:locality}), and it
is likely that these will only get worse in the future.  Hence, moving
to more explicitly-separate heap regions is a more honest reflection
of the underlying memory architecture, and is likely to allow the
implementation to make intelligent decisions about data locality.

\section*{Acknowledgments}

We wish to thank Jost Berthold, who helped us identify some of the
bottlenecks in the parallel runtime implementation, and built the
first implementation of work-stealing queues for spark distribution
during an internship at Microsoft Research in the summer of 2008.

{\tiny
\bibliographystyle{alpha}
\bibliography{multicore-ghc}
}

\end{document}
