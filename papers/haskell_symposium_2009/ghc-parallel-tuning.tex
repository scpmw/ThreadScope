% $Id: ghc-parallel-tuning.tex#1 2009/04/22 10:31:00 REDMOND\\satnams $
% $Source: //depot/satnams/haskell/ThreadScope/papers/haskell_symposium_2009/ghc-parallel-tuning.tex $

\documentclass[twocolumn,9pt]{sigplanconf}

\usepackage{url}
% \usepackage{code}
\usepackage{graphicx}
\usepackage{enumerate}

\usepackage{listings}
\lstset{basicstyle=\fontfamily{cmss} \small, columns=fullflexible, language=Haskell, numbers=left, numberstyle=\tiny, numbersep=2pt}

\newcommand{\codef}[1]{{\fontfamily{cmss}\small#1}}
\newcommand{\boldcode}[1]{{\bf\fontfamily{cmss}\small#1}}


\nocaptionrule

\title{Parallel Performance Tuning for Haskell}

\authorinfo{Donnie Jones}{University of Kentucky}
           {donnie@darthik.com}
\authorinfo{Simon Marlow}{Microsoft Research}
           {simonmar@microsoft.com}
\authorinfo{Satnam Singh}{Microsoft Research}
           {satnams@microsoft.com}

\begin{document}

\maketitle
%\makeatactive

\begin{abstract}
Profiling information is essential for the performance tuning of parallel programs. This paper describes a new parallel profiling mechansim in the GHC system and its associated graphical viewer. We illustrate how this system can be used to help diagnose and fix several common kinds of performance errors in semi-explicit parallel Haskell programs.
\end{abstract}


\section{Introduction}
Semi-explicit parallel programs in Haskell are written using annotations to control the granularity of parallelsim and the squenecing of evaluation~\cite{spj:trin98b}. This leads to deterministic fine-grain parallel programs that can be executed on multi-core computers. However, it can be difficult to observe how effectively a program has been parallelized. This paper presents some recently developed profiling technology in the GHC system which can be used for parallel performanced tuning of Haskell programs.

\section{Background}

\section{Profiling Motivation}
Show examples of semi-explicit parallel programs that go wrong. Show what we could measure before using heap and time profiling and motivate the need for better profiling.

Haskell provides a mechanism to allow the user to control the granularity of parallelism by indicating what computations may be usefully carried out in parallel. This is done by using functions from the \codef{Control.Parallel} module. The interface for \codef{Control.Parallel} is shown below:
\begin{lstlisting}
  par :: a -> b -> b 
  pseq :: a -> b -> b 
\end{lstlisting}
The function \codef{par} indicates to the Haskell run-time system that it may be beneficial to evaluate the first argument in parallel with the second argument. The \codef{par} function returns as its result the value of the second argument. One can always eliminate \codef{par} from a program by using the following identity without altering the semantics of the program:
\begin{lstlisting}
  par a b = b 
\end{lstlisting}
A thread is not necessarily created to compute the value of the expression \codef{a}. Instead, the Haskell run-time system creates a {\em spark} which has the potential to be executed on a different thread from the parent thread. A sparked computation expresses the possibility of performing some speculative evaluation. Since a thread is not necessarily created to compute the value of \codef{a} this approach has some similarities with the notion of a {\em lazy future}~\cite{mohr:91}.

Sometimes it is convenient to write a function with two arguments as an infix function and this is done in Haskell by writing quotes around the function:
\begin{lstlisting}
  a `par` b
\end{lstlisting}

We call such programs semi-explicitly parallel because the programmer has provided a hint about the appropriate level of granularity for parallel operations and the system implicitly creates threads to implement the concurrency. The user does not need to explicitly create any threads or write any code for inter-thread communication or synchronization.

To illustrate the use of \codef{par} we present a program that performs two compute intensive functions in parallel. The first compute intensive function we use is the notorious Fibonacci function:
\begin{lstlisting}
fib :: Int -> Int
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting}
The second compute intensive function we use is the \codef{sumEuler} function taken from~\cite{trinder:02}:
\begin{lstlisting}
mkList :: Int -> [Int]
mkList n = [1..n-1]

relprime :: Int -> Int -> Bool
relprime x y = gcd x y == 1

euler :: Int -> Int
euler n = length (filter (relprime n) (mkList n))

sumEuler :: Int -> Int
sumEuler = sum . (map euler) . mkList
\end{lstlisting}
The function that we wish to parallelize adds the results of calling \codef{fib} and \codef{sumEuler}:
\begin{lstlisting}
sumFibEuler :: Int -> Int -> Int
sumFibEuler a b = fib a + sumEuler b
\end{lstlisting}
As a first attempt we can try to use \codef{par} the speculatively spark off the computation of \codef{fib} while the parent thread works on \codef{sumEuler}:
\begin{lstlisting}
parSumFibEuler :: Int -> Int -> Int
parSumFibEuler a b
  = f `par` (f + e)
    where
    f = fib a
    e = sumEuler b
\end{lstlisting}

\section{Profiling Infrastructure}

The profiling framework has three components:

\begin{itemize}
\item Support in GHC's runtime for tracing events to a log file at
  runtime.  The tracing is designed to be as lightweight as possible,
  so as not to have any significant impact on the behaviour of the
  program being measured.

\item A Haskell library @ghc-events@ that can read the trace file
  generated by the runtime and build a Haskell data structure
  representing the trace.

\item Multiple tools make use of the @ghc-events@ library to read and
  analyse trace files.
\end{itemize}

Having a single trace-file format and a library that parses it means
that it is easy to write a new tool that works with GHC trace files:
just import the @ghc-events@ package and write code that uses the
Haskell data structures directly.  We have already built several such
tools ourselves, some of which are merely proof-of-concept
experiments, but the @ghc-events@ library makes it almost trivial to
create new tools:

\begin{itemize}
\item A trivial program that just prints out the (sorted) contents of
  the trace file as text.  Useful for checking that a trace file can
  be parsed, and for examining the exact sequence of events.

\item The ThreadScope graphical viewer.

\item A tool that parses a trace file and generates a PDF format
  timeline view, similar to the ThreadScope view.

\item A tool that generates input in the format expected by the
  gtkwave circuit waveform viewer.  This was used as an early
  prototype for ThreadScope, since the timeline view that we want to
  display has a lot in common with the waveform diagrams that gtkwave
  displays and browses.
\end{itemize}

\subsection{Fast runtime tracing}

% how much impact does this have on runtimes?
% how fast can we generate an event?
% how many events are there in a typical log file?

\subsection{An extensible file format}

We believe it is essential that the trace file format is both
backwards and forwards compatible, and architecture independent.  In
particular, this means that:

\begin{itemize}
\item If you build a newer version of a tool, it will still work with
  the trace files you already have, and trace files generated by
  programs compiled with older versions of GHC.

\item If you upgrade your GHC and recompile your programs, the trace
  files will still work with any profiling tools you already have.

\item Trace files do not have a shelf life.  You can keep your trace
  files around, safe in the knowledge that they will work with future
  versions of profiling tools.  Trace files can be archived, and
  shared between machines.
\end{itemize}

Nevertheless, we don't expect the form of trace files to remain
completely static.  We expect that in the future we will want to add
new events, and add more information to existing events.  We therefore
need an extensible file format.  Informally, our trace files are
structured as follows:

\begin{itemize}
\item A list of \emph{event types}.  An event-type is a
  variable-length structure that describes one kind of event.  The
  event-type structure contains
  \begin{itemize}
    \item A unique number for this event type
    \item A field describing the length of the event itself, or zero
      for a variable-length event.
    \item A variable-length string describing this event (for example
      ``thread created'')
    \item A variable-length field for future expansion.  We might in
      the future want to add more fields to the event-type structure,
      and this field allows for that.
  \end{itemize}
\item A list of \emph{events}.  Each event begins with an event number
  that corresponds to one of the event types defined earlier, and the
  length of the event structure is given by the event type (or it has
  variable length).  The event also contains
  \begin{itemize}
  \item A nanosecond-resolution timestamp.
  \item For a variable-length event, the length of the event.
  \item Information specific to this event, for example which CPU it
    occurred on.  If the parser knows about this event, then it can
    parse the rest of the event's information, otherwise it can skip
    over this field because its length is known.
  \end{itemize}
\end{itemize}

The unique numbers that identify events are shared knowledge between
GHC and the @ghc-events@ library.  When creating a new event, a new
unique identifier is chosen; identifiers can never be re-used.

Hence, even when parsing a trace file that contains new events, the
parser can still give a timestamp and a description of the unknown
events.  The parser might encounter an event-type that it knows about,
but the event-type might contain new unknown fields.  The parser can
recognise this situation and skip over the extra fields, because it
knows the length of the event from the event-type structure.
Therefore when a tool encounters a new log file it can continue to
provide consistent functionality.

Of course, there are scenarios in which it isn't possible to provide
this ideal graceful degradation.  For example, we might construct a
tool that profiles a particular aspect of the behaviour of the
runtime, and in the future the runtime might be redesigned to behave
in a completely different way, with a new set of events.  The old
events simply won't be generated any more, and the old tool won't be
able to display anything useful with the new trace files.  Still, we
expect that our extensible trace file format will allows us to smooth
over the majority of forwards- and backwards-compatibility issues that
will arise between versions of the tools and GHC runtime.  Moreover,
extensibility costs almost nothing, since the extra fields are all in
the event-types header, which has a fixed size for a given version of
GHC.

\section{Case Studies}
Revist examples and show how performance can be tuned with the information obtained from the profiler.

\subsection{SumEuler}
This example has been used several times before and it is useful for illustrating how sparks should be carefully constructed and the need for pseq.

\input{related-work}

\section{Conclusions}

\bibliographystyle{alpha}
\bibliography{ghc-parallel-tuning}

\end{document}
